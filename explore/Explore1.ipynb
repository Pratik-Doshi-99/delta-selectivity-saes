{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f364599-bb67-4be1-a8fc-aa52536df75a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01d92e6-c4ed-406f-8da3-659746c8ff57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m167.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c689b21-2028-4566-86cf-ddcd3a9180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a53d1-26ad-4e57-ba46-8a568f106af6",
   "metadata": {},
   "source": [
    "## All Datasets - Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99e0e5e8-7293-48c9-863b-ebcf2fa5e9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset from 'data/modelscope': Directory data/modelscope is neither a `Dataset` directory nor a `DatasetDict` directory.\n",
      "1) Dataset: ewt.pyth.512.-1, Length: 1438\n",
      "Data Peek: dict_keys(['Number', 'Mood', 'Tense', 'VerbForm', 'PronType', 'Person', 'NumType', 'Voice', 'Gender', 'eos', 'first_eos', 'upos', 'dep', 'head', 'within_compound_token_ix', 'max_compound_token_ix', 'tokens', 'doc_id', 'split', 'position', 'upos_NOUN|probe_indices', 'upos_NOUN|probe_classes', 'upos_PUNC|probe_indices', 'upos_PUNC|probe_classes', 'upos_ADP|probe_indices', 'upos_ADP|probe_classes', 'upos_NUM|probe_indices', 'upos_NUM|probe_classes', 'upos_SYM|probe_indices', 'upos_SYM|probe_classes', 'upos_SCONJ|probe_indices', 'upos_SCONJ|probe_classes', 'upos_ADJ|probe_indices', 'upos_ADJ|probe_classes', 'upos_DET|probe_indices', 'upos_DET|probe_classes', 'upos_CCONJ|probe_indices', 'upos_CCONJ|probe_classes', 'upos_PROPN|probe_indices', 'upos_PROPN|probe_classes', 'upos_PRON|probe_indices', 'upos_PRON|probe_classes', 'upos_X|probe_indices', 'upos_X|probe_classes', 'upos_ADV|probe_indices', 'upos_ADV|probe_classes', 'upos_INTJ|probe_indices', 'upos_INTJ|probe_classes', 'upos_VERB|probe_indices', 'upos_VERB|probe_classes', 'upos_AUX|probe_indices', 'upos_AUX|probe_classes', 'dep_acl|probe_indices', 'dep_acl|probe_classes', 'dep_acl:relcl|probe_indices', 'dep_acl:relcl|probe_classes', 'dep_advcl|probe_indices', 'dep_advcl|probe_classes', 'dep_advmod|probe_indices', 'dep_advmod|probe_classes', 'dep_amod|probe_indices', 'dep_amod|probe_classes', 'dep_appos|probe_indices', 'dep_appos|probe_classes', 'dep_aux|probe_indices', 'dep_aux|probe_classes', 'dep_aux:pass|probe_indices', 'dep_aux:pass|probe_classes', 'dep_case|probe_indices', 'dep_case|probe_classes', 'dep_cc|probe_indices', 'dep_cc|probe_classes', 'dep_ccomp|probe_indices', 'dep_ccomp|probe_classes', 'dep_compound|probe_indices', 'dep_compound|probe_classes', 'dep_conj|probe_indices', 'dep_conj|probe_classes', 'dep_cop|probe_indices', 'dep_cop|probe_classes', 'dep_det|probe_indices', 'dep_det|probe_classes', 'dep_flat|probe_indices', 'dep_flat|probe_classes', 'dep_list|probe_indices', 'dep_list|probe_classes', 'dep_mark|probe_indices', 'dep_mark|probe_classes', 'dep_nmod|probe_indices', 'dep_nmod|probe_classes', 'dep_nmod:poss|probe_indices', 'dep_nmod:poss|probe_classes', 'dep_nsubj|probe_indices', 'dep_nsubj|probe_classes', 'dep_nsubj:pass|probe_indices', 'dep_nsubj:pass|probe_classes', 'dep_nummod|probe_indices', 'dep_nummod|probe_classes', 'dep_obj|probe_indices', 'dep_obj|probe_classes', 'dep_obl|probe_indices', 'dep_obl|probe_classes', 'dep_parataxis|probe_indices', 'dep_parataxis|probe_classes', 'dep_punct|probe_indices', 'dep_punct|probe_classes', 'dep_root|probe_indices', 'dep_root|probe_classes', 'dep_xcomp|probe_indices', 'dep_xcomp|probe_classes', 'VerbForm_Fin|probe_indices', 'VerbForm_Fin|probe_classes', 'VerbForm_Inf|probe_indices', 'VerbForm_Inf|probe_classes', 'VerbForm_Ger|probe_indices', 'VerbForm_Ger|probe_classes', 'VerbForm_Part|probe_indices', 'VerbForm_Part|probe_classes', 'PronType_Art|probe_indices', 'PronType_Art|probe_classes', 'PronType_Dem|probe_indices', 'PronType_Dem|probe_classes', 'PronType_Prs|probe_indices', 'PronType_Prs|probe_classes', 'PronType_Rel|probe_indices', 'PronType_Rel|probe_classes', 'PronType_Int|probe_indices', 'PronType_Int|probe_classes', 'Person_1|probe_indices', 'Person_1|probe_classes', 'Person_2|probe_indices', 'Person_2|probe_classes', 'Person_3|probe_indices', 'Person_3|probe_classes', 'Gender_Masc|probe_indices', 'Gender_Masc|probe_classes', 'Gender_Fem|probe_indices', 'Gender_Fem|probe_classes', 'Gender_Neut|probe_indices', 'Gender_Neut|probe_classes', 'Number_Plur|probe_indices', 'Number_Plur|probe_classes', 'Mood_Imp|probe_indices', 'Mood_Imp|probe_classes', 'Tense_Past|probe_indices', 'Tense_Past|probe_classes', 'NumType_Card|probe_indices', 'NumType_Card|probe_classes', 'Voice_Pass|probe_indices', 'Voice_Pass|probe_classes', 'eos_True|probe_indices', 'eos_True|probe_classes', 'first_eos_True|probe_indices', 'first_eos_True|probe_classes'])\n",
      "_________________________\n",
      "2) Dataset: latex.pyth.1024.-1, Length: 4486\n",
      "Data Peek: dict_keys(['tokens', 'is_frac|probe_indices', 'is_frac|probe_classes', 'is_frac|valid_indices', 'is_numerator|probe_indices', 'is_numerator|probe_classes', 'is_numerator|valid_indices', 'is_denominator|probe_indices', 'is_denominator|probe_classes', 'is_denominator|valid_indices', 'is_title|probe_indices', 'is_title|probe_classes', 'is_title|valid_indices', 'is_abstract|probe_indices', 'is_abstract|probe_classes', 'is_abstract|valid_indices', 'is_author|probe_indices', 'is_author|probe_classes', 'is_author|valid_indices', 'is_subscript|probe_indices', 'is_subscript|probe_classes', 'is_subscript|valid_indices', 'is_superscript|probe_indices', 'is_superscript|probe_classes', 'is_superscript|valid_indices', 'is_reference|probe_indices', 'is_reference|probe_classes', 'is_reference|valid_indices', 'is_math|probe_indices', 'is_math|probe_classes', 'is_math|valid_indices', 'is_inline_math|probe_indices', 'is_inline_math|probe_classes', 'is_inline_math|valid_indices', 'is_display_math|probe_indices', 'is_display_math|probe_classes', 'is_display_math|valid_indices', 'start_math|probe_indices', 'start_math|probe_classes', 'start_math|valid_indices', 'end_math|probe_indices', 'end_math|probe_classes', 'end_math|valid_indices'])\n",
      "_________________________\n",
      "3) Dataset: compound_words.pyth.24.-1, Length: 167959\n",
      "Data Peek: dict_keys(['tokens', 'label', 'feature_name'])\n",
      "_________________________\n",
      "4) Dataset: distribution_id.pyth.512.-1, Length: 8413\n",
      "Data Peek: dict_keys(['text', 'meta', 'all_tokens', 'tokens', 'distribution', 'probe_indices', 'valid_indices'])\n",
      "_________________________\n",
      "5) Dataset: natural_lang_id.pyth.512.-1, Length: 28084\n",
      "Data Peek: dict_keys(['lang', 'tokens', 'class_ids', 'probe_indices', 'valid_indices'])\n",
      "_________________________\n",
      "6) Dataset: text_features.pyth.256.10000, Length: 10000\n",
      "Data Peek: dict_keys(['text', 'meta', 'all_tokens', 'tokens', 'contains_digit|probe_indices', 'contains_digit|probe_classes', 'all_digits|probe_indices', 'all_digits|probe_classes', 'contains_capital|probe_indices', 'contains_capital|probe_classes', 'leading_capital|probe_indices', 'leading_capital|probe_classes', 'all_capitals|probe_indices', 'all_capitals|probe_classes', 'contains_whitespace|probe_indices', 'contains_whitespace|probe_classes', 'has_leading_space|probe_indices', 'has_leading_space|probe_classes', 'no_leading_space_and_loweralpha|probe_indices', 'no_leading_space_and_loweralpha|probe_classes', 'contains_all_whitespace|probe_indices', 'contains_all_whitespace|probe_classes', 'is_not_alphanumeric|probe_indices', 'is_not_alphanumeric|probe_classes', 'is_not_ascii|probe_indices', 'is_not_ascii|probe_classes'])\n",
      "_________________________\n",
      "7) Dataset: programming_lang_id.pyth.512.100, Length: 71\n",
      "Data Peek: dict_keys(['text', 'meta', 'lang_prob', 'lang', 'all_tokens', 'tokens', 'class_ids', 'probe_indices', 'valid_indices'])\n",
      "_________________________\n",
      "8) Dataset: wikidata_sorted_is_alive.pyth.128.6000, Length: 6000\n",
      "Data Peek: dict_keys(['tokens', 'name', 'text', 'name_index_start', 'name_index_end', 'surname_index_start', 'surname_index_end', 'class'])\n",
      "_________________________\n",
      "9) Dataset: wikidata_sorted_occupation.pyth.128.6000, Length: 5980\n",
      "Data Peek: dict_keys(['tokens', 'name', 'text', 'name_index_start', 'name_index_end', 'surname_index_start', 'surname_index_end', 'class'])\n",
      "_________________________\n",
      "10) Dataset: wikidata_sorted_sex_or_gender.pyth.128.6000, Length: 6000\n",
      "Data Peek: dict_keys(['tokens', 'name', 'text', 'name_index_start', 'name_index_end', 'surname_index_start', 'surname_index_end', 'class'])\n",
      "_________________________\n",
      "11) Dataset: wikidata_sorted_political_party.pyth.128.3000, Length: 3000\n",
      "Data Peek: dict_keys(['tokens', 'name', 'text', 'name_index_start', 'name_index_end', 'surname_index_start', 'surname_index_end', 'class'])\n",
      "_________________________\n",
      "12) Dataset: wikidata_sorted_occupation_athlete.pyth.128.5000, Length: 5000\n",
      "Data Peek: dict_keys(['tokens', 'name', 'text', 'name_index_start', 'name_index_end', 'surname_index_start', 'surname_index_end', 'class'])\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Replace this with the path to your base directory containing dataset directories.\n",
    "base_dir = \"data\"\n",
    "count = 1\n",
    "# Iterate over every item in the base directory.\n",
    "peek = []\n",
    "targets = ['','', '', 'feature_name', 'distribution','class_ids','contains_digit|probe_classes','class_ids', 'class','class','class','class','class']\n",
    "for dataset_name in os.listdir(base_dir):\n",
    "    dataset_path = os.path.join(base_dir, dataset_name)\n",
    "    # Check if the item is a directory.\n",
    "    if os.path.isdir(dataset_path):\n",
    "        try:\n",
    "            # Load the dataset from the directory.\n",
    "            ds = load_from_disk(dataset_path)\n",
    "            print(f\"{count}) Dataset: {dataset_name}, Length: {len(ds)}\")\n",
    "            #a = set()\n",
    "            #if targets[count]:\n",
    "            #    for p in ds:\n",
    "            #        a.add(p[targets[count]])\n",
    "            #print('Classes:', a)\n",
    "            print('Data Peek:', ds[0].keys())\n",
    "            print('_'*25)\n",
    "            count += 1\n",
    "            peek.append(ds)\n",
    "            # Iterate over the splits (e.g., train, test, validation) and print their lengths.\n",
    "            #for split, split_dataset in ds.items():\n",
    "            #    print(f\"  {split} split length: {len(split_dataset)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset from '{dataset_path}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861654c6-b961-4bd0-baed-b5a268cc9b67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wikipedia Sex Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ebd0d1-44a8-4782-bcad-0b6883437c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 6000\n"
     ]
    }
   ],
   "source": [
    "data = peek[9]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class']] = unique_count.get(d['class'], 0) + 1\n",
    "    unique_keys.update(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df3d4a6-8761-41c6-a137-8f9cc6830876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'female': 3000, 'male': 3000}\n",
      "{'surname_index_end', 'tokens', 'text', 'class', 'name_index_start', 'name_index_end', 'surname_index_start', 'name'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6babdfe3-744d-4e38-b3db-f928c7d8e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Tokens [126, 127]\n",
      "{'tokens': tensor([    0,    15,  5131,  3159,    14,  9188,    84,   651,   320,  7088,\n",
      "         4661,   254,    13, 17215,    13,   285, 15541,    15,   187,   187,\n",
      "          510,  7015, 18177,   273,   667,  1677, 17502,   310,  1900,   253,\n",
      "         5968,   344, 10316,   281,   253,  8310,   273,  8801,   414,   285,\n",
      "        13375,  1128,   783, 12315,   562,   273,   534,   344,  8203, 13436,\n",
      "          521,  5705,    15,  1737,  2238,   273,  2660,   285,   752,  2408,\n",
      "          273,  6009,  1304,   841,  1841,    32,  5863,   281,   849,  3332,\n",
      "          247,  3522,   310,   253,  4498,  4824,    32,  1737,   253,  6500,\n",
      "           13,   285,  2714,  7393,  1632,   273,   253, 16057,  1128,  5371,\n",
      "          521,   246, 18909,   273, 35633,    32,   380,  1390,  1318,   273,\n",
      "        21518,  3890,   398,    13,  2469,   285,  1246,  1128, 45924, 27119,\n",
      "          296,   248,  5298,    13,  1608,  8765, 44929,  1128,   263,   275,\n",
      "          776,  1211,  1388, 12362, 28533,    13,  9964, 36883]), 'name': 'Victor Hugo', 'text': '. Other word-signs would be Good Cheer, Content, and Hope.\\n\\nThe chief trait of any given poet is always the spirit he brings to the observation of Humanity and Nature—the mood out of which he contemplates his subjects. What kind of temper and what amount of faith report these things? Up to how recent a date is the song carried? What the equipment, and special raciness of the singer—what his tinge of coloring? The last value of artistic expressers, past and present—Greek æsthetes, Shakspere—or in our own day Tennyson, Victor Hugo', 'name_index_start': 126, 'name_index_end': 127, 'surname_index_start': 127, 'surname_index_end': 127, 'class': 'male'}\n",
      "_________________________\n",
      "tensor([ 9964, 36883])\n",
      "_________________________\n",
      "| Victor Hugo|\n",
      "_________________________\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "sample = data[9]\n",
    "\n",
    "name_range = list(range(sample['name_index_start'],sample['name_index_end']+1))\n",
    "print('Name Tokens',name_range)\n",
    "print(sample)\n",
    "print('_'*25)\n",
    "print(sample['tokens'][name_range])\n",
    "print('_'*25)\n",
    "print(f\"|{tokenizer.decode(sample['tokens'][name_range])}|\")\n",
    "print('_'*25)\n",
    "print(sample['class'])\n",
    "#print(sample['text'][name_range])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79fe59-cbfe-4968-8db1-9d4828ae4b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wikipedia Political Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008298f2-42d2-48d8-9ff0-2d6f7fcdf21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 3000\n"
     ]
    }
   ],
   "source": [
    "data = peek[10]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class']] = unique_count.get(d['class'], 0) + 1\n",
    "    unique_keys.update(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae8f733-b703-4226-96ee-66cedd429844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Democratic Party': 1500, 'Republican Party': 1500}\n",
      "{'surname_index_end', 'tokens', 'text', 'class', 'name_index_start', 'name_index_end', 'surname_index_start', 'name'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3858a46-8ddd-44a1-ac4a-40c5573a47c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Tokens [122, 123, 124, 125, 126, 127]\n",
      "{'tokens': tensor([    0,  2717,   281,  6215,   562,    13,   387,  1878,   281,   253,\n",
      "         6070,   326,   352,   651,  1056,   352, 32811,   281,  2075,   253,\n",
      "         4815,    15,   187,   187,     3, 33832, 14761,  1320,   273, 40459,\n",
      "          285, 38042, 10260, 35479,  1110, 13120, 16936,   285,  4934,  4815,\n",
      "         1698,    15, 13480, 29488,   858,   417,  4763,  2208,  8385,   846,\n",
      "          253,  2137,    15,   496,   958,    13,   247,  2137,    14,  2606,\n",
      "         1458,     6,  2507,   885,  2891,   327, 14997,  5486,   281, 43162,\n",
      "        46044, 21731,  6194,  4288,  6376,   275,  1055,    13,  7562,  4195,\n",
      "          373,  2169,   685,  3309,   449,   187,   187,    42,  1353, 29985,\n",
      "          326, 16612,  1346,   323, 39995,    13,  1125,   375,    13,  4166,\n",
      "           13,   285, 26429,   778,   452,   574,   247,  1133,   275,   352,\n",
      "           15,   187,   187,  4576,  1311,  2300,  3489,    48,  1905,  2091,\n",
      "        21806,    15, 25689,  3697,  7523,  5282,   909,  1063]), 'name': 'Arnold Schwarzenegger', 'text': ' nothing to ship out, at least to the extent that it would make it worthwhile to pay the costs.\\n\\n\"Government subsidization of highways and airports greatly aided those transportation developments and kept costs low. Railroads did not receive government assistance after the war. In fact, a war-time 15% excise tax on tickets meant to discourage wartime civilian train travel remained in effect, keeping fares higher than necessary.\"\\n\\nI\\'m guessing that lobbyists for airlines, autos, oil, and tires may have had a hand in it.\\n\\nSACRAMENTO — When Gov. Arnold Schwarzenegger', 'name_index_start': 122, 'name_index_end': 127, 'surname_index_start': 123, 'surname_index_end': 127, 'class': 'Republican Party'}\n",
      "_________________________\n",
      "tensor([25689,  3697,  7523,  5282,   909,  1063])\n",
      "_________________________\n",
      "| Arnold Schwarzenegger|\n",
      "_________________________\n",
      "|Republican Party|\n"
     ]
    }
   ],
   "source": [
    "sample = data[10]\n",
    "\n",
    "name_range = list(range(sample['name_index_start'],sample['name_index_end']+1))\n",
    "print('Name Tokens',name_range)\n",
    "print(sample)\n",
    "print('_'*25)\n",
    "print(sample['tokens'][name_range])\n",
    "print('_'*25)\n",
    "print(f\"|{tokenizer.decode(sample['tokens'][name_range])}|\")\n",
    "print('_'*25)\n",
    "print(f\"|{sample['class']}|\")\n",
    "#print(sample['text'][name_range])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86676831-b010-4eac-9a86-3349dd70ae0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wikipedia Athlete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f1f5c99-bfa2-408d-8f81-f5c3d28e1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 5000\n"
     ]
    }
   ],
   "source": [
    "data = peek[11]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class']] = unique_count.get(d['class'], 0) + 1\n",
    "    unique_keys.update(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358ca995-ac5e-46c3-a0a0-9c844d108eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'association football player': 1000, 'basketball player': 1000, 'baseball player': 1000, 'American football player': 1000, 'ice hockey player': 1000}\n",
      "{'surname_index_end', 'tokens', 'text', 'class', 'name_index_start', 'name_index_end', 'surname_index_start', 'name'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e43e057-ca3c-4ee2-96f5-eece5178e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Tokens [126, 127]\n",
      "{'tokens': tensor([    0,  2626,   285, 13268, 30416,   267, 11691,   337,    27,  2504,\n",
      "         1996,   281,  1918,  9756,   697,  7002,  4951,  3330,    15, 12705,\n",
      "        15858,   438,   706,   249,  1160,  3307, 26866,   275,   253, 10170,\n",
      "           15,   187,   187,  3848, 18415,  5602,    44, 32560,   285,   416,\n",
      "           15,    43,    15,   530,  2480,  1063,  1097, 11691,   323,   253,\n",
      "        10063,  5332,  1507,    13,   665,  8231,   616,  1273,  4951,   846,\n",
      "         2876,   272,  2366,   271,  4314,    14, 13197, 29642,    15, 35176,\n",
      "           74,  8679, 18540, 13456,  4136,   512,  1264,  7342,   327,  4791,\n",
      "        13768,   275,   253,   873,  2135,    15,   187,   187,     3,  1231,\n",
      "         3534,   598,  1264,  7342,   275,  1679,   685,   495,  2909,    15,\n",
      "         1198,  8988,  2909,    13,   309,  1869,   359,  7303,   352,  3965,\n",
      "          973,    15,  9240,  1264,  2909,    13,   326,   434,   253,  2165,\n",
      "          937, 10063,  5332,  1507,  1481,  8690, 24010, 35122]), 'name': 'Todd Richards', 'text': ' third and Jordan Staal scored 1:47 later to give Carolina its fourth straight win. Anton Khudobin made 22 saves in the victory.\\n\\nDerek MacKenzie and R.J. Umberger both scored for the Blue Jackets, who dropped their second straight after stringing together an eight-game streak. Sergei Bobrovsky allowed all three goals on 35 shots in the setback.\\n\\n\"We gave up three goals in less than 3 minutes. For 57 minutes, I thought we managed it pretty well. Those three minutes, that\\'s the game,\" Blue Jackets head coach Todd Richards', 'name_index_start': 126, 'name_index_end': 127, 'surname_index_start': 127, 'surname_index_end': 127, 'class': 'ice hockey player'}\n",
      "_________________________\n",
      "tensor([24010, 35122])\n",
      "_________________________\n",
      "| Todd Richards|\n",
      "_________________________\n",
      "ice hockey player\n"
     ]
    }
   ],
   "source": [
    "sample = data[2540]\n",
    "\n",
    "name_range = list(range(sample['name_index_start'],sample['name_index_end']+1))\n",
    "print('Name Tokens',name_range)\n",
    "print(sample)\n",
    "print('_'*25)\n",
    "print(sample['tokens'][name_range])\n",
    "print('_'*25)\n",
    "print(f\"|{tokenizer.decode(sample['tokens'][name_range])}|\")\n",
    "print('_'*25)\n",
    "print(sample['class'])\n",
    "#print(sample['text'][name_range])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec7d03-8797-4eb4-b14e-2834f8c14962",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75a0a2b2-566d-4052-aecd-f45ae8a24710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 6000\n",
      "{'true': 3000, 'false': 3000}\n",
      "{'surname_index_end', 'tokens', 'text', 'class', 'name_index_start', 'name_index_end', 'surname_index_start', 'name'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[7]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class']] = unique_count.get(d['class'], 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a06569d9-c7e6-4b52-9692-89f2e8808784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Tokens [35, 36, 37]\n",
      "{'tokens': tensor([    0,    46, 46929,   556, 26582, 31734,    15,  2053,   513,   417,\n",
      "         4833, 21977,  2600,    13,  2167,   353, 46929,   778,  6233, 36662,\n",
      "          323,  3580,  9716,  3066, 26582,  4859,    15,   187,   187, 13117,\n",
      "        17416,  1511, 11412,   187,   187, 40616,   443, 12727,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'name': 'Lady Gaga', 'text': 'Meredith has affiliate partnerships. These do not influence editorial content, though Meredith may earn commissions for products purchased via affiliate links.\\n\\nJoanne type Music\\n\\nLady Gaga<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', 'name_index_start': 35, 'name_index_end': 37, 'surname_index_start': 36, 'surname_index_end': 37, 'class': 'true'}\n",
      "_________________________\n",
      "tensor([40616,   443, 12727])\n",
      "_________________________\n",
      "|Lady Gaga|\n",
      "_________________________\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "sample = data[0]\n",
    "\n",
    "name_range = list(range(sample['name_index_start'],sample['name_index_end']+1))\n",
    "print('Name Tokens',name_range)\n",
    "print(sample)\n",
    "print('_'*25)\n",
    "print(sample['tokens'][name_range])\n",
    "print('_'*25)\n",
    "print(f\"|{tokenizer.decode(sample['tokens'][name_range])}|\")\n",
    "print('_'*25)\n",
    "print(sample['class'])\n",
    "#print(sample['text'][name_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01144d92-bae0-411a-8b8e-774f9128cc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if sample['class'] == 'true' else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b9f5d-df76-462d-8c18-264b8214d750",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16f7af8a-f2f9-41f4-ac16-65aa132dfae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 5980\n",
      "{'singer': 1000, 'actor': 1000, 'politician': 1000, 'journalist': 1000, 'athlete': 1000, 'researcher': 980}\n",
      "{'surname_index_end', 'tokens', 'text', 'class', 'name_index_start', 'name_index_end', 'surname_index_start', 'name'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[8]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class']] = unique_count.get(d['class'], 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16200c0f-b37e-4eda-aca2-65adb25dfad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Tokens [126, 127]\n",
      "{'tokens': tensor([    0,   745,   342,   253, 14511,    15, 17814,   310,  1024, 13644,\n",
      "        29075,  3006,   670,   849, 16922, 21088,  7902,   457,    84,  7185,\n",
      "          285,   611,  1279,    70,  4255,   457,    84,   773,  4924,  1869,\n",
      "          668,   588, 42620,  2806, 11163,   689,   281,   253,  8786,  7021,\n",
      "           15,   535,   187,   688,   271,  1121,    14,   264,   323,   253,\n",
      "        17372,  7648,  5687, 18879,    13,   773, 16008,  3778,  1680,   411,\n",
      "         3288,   272,  5418,   657,   302,   398,   285, 43779,   253,  9922,\n",
      "         7021,  1806, 33614,  2955,   282, 32920, 12013,    27,   187,   187,\n",
      "        16008,  3778,   310, 42547,   387,  1633,  1643,   952,  1869,  1896,\n",
      "           15,   754,   310,  2970,   690,  2806,   952,   281,  7277,   779,\n",
      "          281, 22306,  6729,    15,  6729,  1904,   457,    85,   755, 16922,\n",
      "         7902,   562,   273, 12907,    15,  6729,  1904,   457,    85, 34030,\n",
      "         5332,   544, 32336,  1092,  1014,  2167, 11290, 32574]), 'name': 'Ken Burns', 'text': ' off with the narrative. Everyone is now publicly theorizing about how Alice Marie Johnson’s freedom and Kanye West’s “free thought” will lure black voters over to the Republican Party.\\n\\n\\nIn an op-ed for the Detroit Free Press titled, “Donald Trump Is Wooing Black Voters and Killing the Democratic Party,” Rochelle Riley writes:\\n\\nDonald Trump is succeeding at something few people thought possible. He is getting some black people to compare him to Barack Obama. Obama didn’t get Alice Johnson out of jail. Obama didn’t pardon Jack [Johnson], even though Ken Burns', 'name_index_start': 126, 'name_index_end': 127, 'surname_index_start': 127, 'surname_index_end': 127, 'class': 'actor'}\n",
      "_________________________\n",
      "tensor([11290, 32574])\n",
      "_________________________\n",
      "| Ken Burns|\n",
      "_________________________\n",
      "|actor|\n"
     ]
    }
   ],
   "source": [
    "sample = data[2000]\n",
    "\n",
    "name_range = list(range(sample['name_index_start'],sample['name_index_end']+1))\n",
    "print('Name Tokens',name_range)\n",
    "print(sample)\n",
    "print('_'*25)\n",
    "print(sample['tokens'][name_range])\n",
    "print('_'*25)\n",
    "print(f\"|{tokenizer.decode(sample['tokens'][name_range])}|\")\n",
    "print('_'*25)\n",
    "print(f\"|{sample['class']}|\")\n",
    "#print(sample['text'][name_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "460d8928-f2a3-4cc4-a14d-6bd28fd95e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['class'] == 'actor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3c1eb-2bb2-46b4-bcca-ee8bcbf6b43d",
   "metadata": {},
   "source": [
    "## Natural Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dafe0e-d896-41c1-9690-59438abd196f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 28084\n",
      "{4: 3137, 9: 3097, 2: 3090, 1: 3102, 3: 3126, 7: 2967, 5: 3155, 8: 3110, 6: 3300}\n",
      "{'tokens', 'lang', 'probe_indices', 'valid_indices', 'class_ids'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[4]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['class_ids'].item()] = unique_count.get(d['class_ids'].item(), 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ab5a50d-45fe-42b2-9e54-90b54cae8a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "Tokens shape torch.Size([512])\n",
      "Valid indices shape torch.Size([510])\n",
      "_________________________\n",
      "Decoded tokens at valid indices : μβουλίου για λίγον καιρό. Στον περιορισμένο χρόνο που έχετε στη διάθεσή σας, δεν μπορείτε να επιτύχετε κάτι μεγάλο, παρ’ όλ’ αυτά μπορείτε να δώσετε κατευθύνσεις σε κάποια θέματα όπως η ενέργεια και η επικουρικότητα. Μπορείτε επίσης να κάνετε κάτι για τα οικονομικά. Σας παρακαλώ, όμως, μην αφήσετε να σας παραπλανήσουν εκείνοι που είναι ιστορικά αποδεδειγμένο πως δεν μπορούν να διαχειριστούν χρήματα. Μην πιστέψετε ότι με περισσότερα χρήματα μπορεί κανείς αυτόματα να επιτύχει καλύτερα πράγματα. Χρήματα άλλωστε υπάρχουν, έχουμε 112 δισ. ευρώ. Το θέμα είναι μόνο τι θα τα κάνουμε. Είπατε ο ίδιος, κύριε Καγκελάριε, ότι το σημαντικότερο είναι η ερώτηση. Είπατε ο ίδιος πως πρέπει να δημιουργηθεί εμπιστοσύνη. Κάντε το λοιπόν! Περάστε στην πράξη! Πείτε ποιος θα πάρει τι, ποιος θα πληρώσει τι, από τα χρήματα της Ευρώπης.\n",
      "Πόσες οικονομικές ενισχύσεις έλαβαν μέχρι σήμερα ο σημαντικός μεγιστάνας των τραπε\n",
      "_________________________\n",
      "Tokens at probe index: tensor([14858, 18806])\n",
      "Decoded tokens at probe index:\n",
      "|ια|\n",
      "| και|\n",
      "class index: 6\n"
     ]
    }
   ],
   "source": [
    "sample = data[25621]\n",
    "\n",
    "print('_'*25)\n",
    "print('Tokens shape',sample['tokens'].shape)\n",
    "print('Valid indices shape', sample['valid_indices'].shape)\n",
    "print('_'*25)\n",
    "print('Decoded tokens at valid indices :',tokenizer.decode(sample['tokens'][sample['valid_indices']]))\n",
    "print('_'*25)\n",
    "print('Tokens at probe index:',sample['tokens'][sample['probe_indices']])\n",
    "print('Decoded tokens at probe index:')\n",
    "for t in sample['tokens'][sample['probe_indices']]:\n",
    "    print(f\"|{tokenizer.decode(t)}|\")\n",
    "print('class index:',sample['class_ids'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8f3dfb-294e-4b54-ab16-2434941528e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
       "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
       "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
       "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
       "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
       "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
       "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
       "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
       "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
       "        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
       "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
       "        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
       "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
       "        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
       "        505, 506, 507, 508, 509, 510])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['valid_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f59412-5808-4774-b86c-5233f9873225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([135, 485])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to select probe indices + 50 random positions\n",
    "sample['probe_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12a5dd3a-8a5c-4b82-9991-dfc8f3fe8da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = list(set(torch.randint(low=0, high=len(sample['tokens']), size=(55,)).tolist() + sample['probe_indices'].tolist()))\n",
    "for t in sample['probe_indices']:\n",
    "    assert t in pos, f\"Probe index {t} not in positions\"\n",
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e25c5c5-cf88-4030-aec7-4654c21a410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "485 in pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43a9f6-b185-4b6c-9652-9226ef428e73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f1a918-e514-482d-a114-f72cf5e291c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 10000\n",
      "{}\n",
      "{'leading_capital|probe_classes', 'all_capitals|probe_indices', 'contains_digit|probe_indices', 'is_not_alphanumeric|probe_indices', 'contains_all_whitespace|probe_classes', 'no_leading_space_and_loweralpha|probe_indices', 'all_tokens', 'contains_capital|probe_indices', 'is_not_alphanumeric|probe_classes', 'text', 'leading_capital|probe_indices', 'all_digits|probe_indices', 'tokens', 'meta', 'contains_whitespace|probe_indices', 'has_leading_space|probe_classes', 'is_not_ascii|probe_indices', 'contains_whitespace|probe_classes', 'contains_all_whitespace|probe_indices', 'no_leading_space_and_loweralpha|probe_classes', 'is_not_ascii|probe_classes', 'contains_digit|probe_classes', 'has_leading_space|probe_indices', 'contains_capital|probe_classes', 'all_capitals|probe_classes', 'all_digits|probe_classes'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[5]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    #unique_count[d['is_not_ascii|probe_classes'].item()] = unique_count.get(d['is_not_ascii|probe_classes'].item(), 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b7d821-924a-49b8-98ce-0d641c1796ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_keys) == len(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c80b7925-c6cd-460b-ba08-fdf0a2c1764b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Pedro Henrique Botelho\\n\\nPedro Henrique Botelho (born April 4, 1987) is a Brazilian football player currently playing for FK Sūduva in Lithuania.\\n\\nHe previously played in one Russian Premier League match for FC Krylia Sovetov Samara during the 2007 season.\\n\\nReferences\\n\\nCategory:1987 births\\nCategory:Living people\\nCategory:Brazilian footballers\\nCategory:Brazilian expatriate footballers\\nCategory:Expatriate footballers in Lithuania\\nCategory:FC Krylia Sovetov Samara players\\nCategory:Expatriate footballers in Russia\\nCategory:Russian Premier League players\\nCategory:Expatriate footballers in Latvia\\n\\nCategory:Association football defenders',\n",
       " 'meta': {'pile_set_name': 'Wikipedia (en)'},\n",
       " 'all_tokens': tensor([49789,   287, 33294,  1452, 28469,   293,  1689,   187,   187, 49789,\n",
       "           287, 33294,  1452, 28469,   293,  1689,   313,  6448,  4162,   577,\n",
       "            13, 12034,    10,   310,   247, 24110,  5842,  4760,  4390,  4882,\n",
       "           323, 46789,   322, 15248,   563,  6156,   275, 40115,   571,    15,\n",
       "           187,   187,  1328,  3786,  4546,   275,   581,  7247, 19425,  6884,\n",
       "          3761,   323, 16004,   611,   610, 19702, 10968,   292,   729,  5769,\n",
       "          4595,  1309,   253,  5215,  2952,    15,   187,   187,  4941,   187,\n",
       "           187,  1413,    27, 15089, 10782,   187,  1413,    27, 14316,   952,\n",
       "           187,  1413,    27, 41946,   757, 23236,   187,  1413,    27, 41946,\n",
       "           757, 44708, 23236,   187,  1413,    27,  1672, 44001,   366, 23236,\n",
       "           275, 40115,   571,   187,  1413,    27,  6739,   611,   610, 19702,\n",
       "         10968,   292,   729,  5769,  4595,  3773,   187,  1413,    27,  1672,\n",
       "         44001,   366, 23236,   275,  7422,   187,  1413,    27, 27397, 19425,\n",
       "          6884,  3773,   187,  1413,    27,  1672, 44001,   366, 23236,   275,\n",
       "         14310, 13917,   187,   187,  1413,    27, 28521,  5842, 31342]),\n",
       " 'tokens': tensor([    0, 49789,   287, 33294,  1452, 28469,   293,  1689,   187,   187,\n",
       "         49789,   287, 33294,  1452, 28469,   293,  1689,   313,  6448,  4162,\n",
       "           577,    13, 12034,    10,   310,   247, 24110,  5842,  4760,  4390,\n",
       "          4882,   323, 46789,   322, 15248,   563,  6156,   275, 40115,   571,\n",
       "            15,   187,   187,  1328,  3786,  4546,   275,   581,  7247, 19425,\n",
       "          6884,  3761,   323, 16004,   611,   610, 19702, 10968,   292,   729,\n",
       "          5769,  4595,  1309,   253,  5215,  2952,    15,   187,   187,  4941,\n",
       "           187,   187,  1413,    27, 15089, 10782,   187,  1413,    27, 14316,\n",
       "           952,   187,  1413,    27, 41946,   757, 23236,   187,  1413,    27,\n",
       "         41946,   757, 44708, 23236,   187,  1413,    27,  1672, 44001,   366,\n",
       "         23236,   275, 40115,   571,   187,  1413,    27,  6739,   611,   610,\n",
       "         19702, 10968,   292,   729,  5769,  4595,  3773,   187,  1413,    27,\n",
       "          1672, 44001,   366, 23236,   275,  7422,   187,  1413,    27, 27397,\n",
       "         19425,  6884,  3773,   187,  1413,    27,  1672, 44001,   366, 23236,\n",
       "           275, 14310, 13917,   187,   187,  1413,    27, 28521,  5842, 31342,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'contains_digit|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'contains_digit|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'all_digits|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'all_digits|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'contains_capital|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'contains_capital|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'leading_capital|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'leading_capital|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'all_capitals|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'all_capitals|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'contains_whitespace|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'contains_whitespace|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'has_leading_space|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'has_leading_space|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'no_leading_space_and_loweralpha|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'no_leading_space_and_loweralpha|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'contains_all_whitespace|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'contains_all_whitespace|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'is_not_alphanumeric|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'is_not_alphanumeric|probe_classes': tensor([], dtype=torch.int64),\n",
       " 'is_not_ascii|probe_indices': tensor([], dtype=torch.int64),\n",
       " 'is_not_ascii|probe_classes': tensor([], dtype=torch.int64)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38c7ca-1962-44a2-8b93-77e48547b1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec1c3349-9baa-4fc9-896a-a606b8f07600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112, 93]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[3265]\n",
    "\n",
    "list(set(torch.concat([sample['contains_digit|probe_indices'],sample['all_digits|probe_indices'], sample['contains_capital|probe_indices'],\n",
    " sample['leading_capital|probe_indices'],sample['all_capitals|probe_indices'],sample['contains_whitespace|probe_indices'],\n",
    "sample['has_leading_space|probe_indices'],sample['no_leading_space_and_loweralpha|probe_indices'],sample['contains_all_whitespace|probe_indices'],\n",
    " sample['is_not_alphanumeric|probe_indices'],sample['is_not_ascii|probe_indices']]).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5544628-d79c-4ab2-beb4-bf92c6ed6dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roman Catholic Diocese of Tambacounda\\n\\nThe Roman Catholic Diocese of Tambacounda () is a diocese located in the city of Tambacounda in the Ecclesiastical province of Dakar in Senegal.\\n\\nHistory\\n August 13, 1970: Established as Apostolic Prefecture of Tambacounda from the Diocese of Kaolack and Diocese of Saint-Louis du Sénégal\\n April 17, 1989: Promoted as Diocese of Tambacounda\\n\\nSpecial churches\\n The cathedral is Cathédrale Marie Reine de l’Univers in Tambacounda, which is located in the Medina Coura neighborhood of the town.\\n\\nLeadership\\n Bishops of Tambacounda (Roman rite)\\n Bishop Jean-Noël Diouf (since 1989.04.17)\\n Prefects Apostolic of Tambacounda (Roman rite) \\n Fr. Clément Cailleau, C.S.Sp. (1970.08.13 – 1986.04.24)\\n\\nSee also\\nRoman Catholicism in Senegal\\n\\nReferences\\n\\nExternal links\\n GCatholic.org\\n Catholic Hierarchy \\n\\nCategory:Roman Catholic dioceses in Senegal\\nCategory:Tambacounda\\nCategory:Christian organizations established in 1970\\nCategory:Roman Catholic dioceses and prelatures established in the 20th century'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample['all_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc3213-9164-431f-bb8e-87143a94ca32",
   "metadata": {},
   "source": [
    "## Data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ca88cdd-2ae9-469e-ba98-c19dbe8ec730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 8413\n",
      "{'wikipedia': 730, 'pubmed_abstracts': 69, 'stack_exchange': 869, 'github': 1068, 'arxiv': 1905, 'uspto': 1056, 'freelaw': 1560, 'hackernews': 696, 'enron': 460}\n",
      "{'tokens', 'distribution', 'text', 'probe_indices', 'all_tokens', 'valid_indices', 'meta'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[3]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['distribution']] = unique_count.get(d['distribution'], 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6307460-b3e9-4ca2-b5c2-d37fecf1c124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens torch.Size([512])\n",
      "<|endoftext|> in 2005. The group has recorded two albums and has received the Sona 9 award, organized by Television of Catalonia, Catalunya Radio and the magazine Enderrock (readers chose the group as the best direct folk of 2007).\n",
      "\n",
      "Filmography\n",
      " Living Is Easy with Eyes Closed (2013), \n",
      " VI premis Barcelona de cinema (2007)\n",
      "IV premios Max de las artes escénicas (2001).2000-2001) Plays Fredrika Armfeldt in \"A Little Night Music\", by Stephen Sondheim and Hugh Wheeler. Dir. Mario Gas.\n",
      " (2004) Special Performance of \"Monólogos de la vagina\", by Eve Ensler.\n",
      " (2009) Special Performance of \"La nit més clara\", by Clara Peya and Amadeu Bergés.\n",
      " (2009) Plays Srta. Mon in \"Groucho me enseñó su camiseta\", by Manuel Vázquez Montalbán and Damià Barbany. Dir. Damià Barbany.\n",
      " (2009) Play: \"En viu\", by Anna Maleras.\n",
      " (2009) Tribute: \"Homenaje a Manuel Gas\". Dir. Mario Gas.\n",
      " (2010) Plays Magda in \"Rock 'n' Roll\", by Tom Stoppard. Dir. Àlex Rigola.\n",
      " (2010-2011) Plays Dama visitante/La reina María Antonieta in \"Beaumarchais\", by Sacha Guitry. Dir. Josep Maria Flotats.\n",
      " (2011) Plays La donzella in \"El comte Arnau\", by Joan Maragall. Dir. Hermann Bonnín.\n",
      " (2012) Plays María in \"Orquesta Club Virginia\", by Manuel Iborra. Dir. Manuel Iborra.\n",
      " (2012) Plays Dama 1 in \"En la vida todo es verdad y todo es mentira\", by Pedro Calderón de la Barca. Dir. Ernesto Caballero.\n",
      " (2012) Benefit Gala: \"No esteu sols!\".\n",
      " (2012) Plays Pepita Troya in \"Doña Perfecta\", by Benito Pérez Galdós. Dir. Ernesto Caballero.\n",
      " (2013-2014) Plays Rain/Gloria in \"Maridos y mujeres\", by Woody Allen\n",
      "_________________________\n",
      "Valid Tokens torch.Size([486])\n",
      ", Catalunya Radio and the magazine Enderrock (readers chose the group as the best direct folk of 2007).\n",
      "\n",
      "Filmography\n",
      " Living Is Easy with Eyes Closed (2013), \n",
      " VI premis Barcelona de cinema (2007)\n",
      "IV premios Max de las artes escénicas (2001).2000-2001) Plays Fredrika Armfeldt in \"A Little Night Music\", by Stephen Sondheim and Hugh Wheeler. Dir. Mario Gas.\n",
      " (2004) Special Performance of \"Monólogos de la vagina\", by Eve Ensler.\n",
      " (2009) Special Performance of \"La nit més clara\", by Clara Peya and Amadeu Bergés.\n",
      " (2009) Plays Srta. Mon in \"Groucho me enseñó su camiseta\", by Manuel Vázquez Montalbán and Damià Barbany. Dir. Damià Barbany.\n",
      " (2009) Play: \"En viu\", by Anna Maleras.\n",
      " (2009) Tribute: \"Homenaje a Manuel Gas\". Dir. Mario Gas.\n",
      " (2010) Plays Magda in \"Rock 'n' Roll\", by Tom Stoppard. Dir. Àlex Rigola.\n",
      " (2010-2011) Plays Dama visitante/La reina María Antonieta in \"Beaumarchais\", by Sacha Guitry. Dir. Josep Maria Flotats.\n",
      " (2011) Plays La donzella in \"El comte Arnau\", by Joan Maragall. Dir. Hermann Bonnín.\n",
      " (2012) Plays María in \"Orquesta Club Virginia\", by Manuel Iborra. Dir. Manuel Iborra.\n",
      " (2012) Plays Dama 1 in \"En la vida todo es verdad y todo es mentira\", by Pedro Calderón de la Barca. Dir. Ernesto Caballero.\n",
      " (2012) Benefit Gala: \"No esteu sols!\".\n",
      " (2012) Plays Pepita Troya in \"Doña Perfecta\", by Benito Pérez Galdós. Dir. Ernesto Caballero.\n",
      " (2013-2014) Plays Rain/Gloria in \"Maridos y mujeres\", by Woody\n",
      "_________________________\n",
      "Tokens to Probe torch.Size([2])\n",
      "tensor([457, 489])\n",
      "[' Pep', '-']\n",
      "_________________________\n",
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "sample = data[11]\n",
    "\n",
    "print('Tokens',sample['tokens'].shape)\n",
    "print(tokenizer.decode(sample['tokens']))\n",
    "print('_'*25)\n",
    "print('Valid Tokens',sample['valid_indices'].shape)\n",
    "print(tokenizer.decode(sample['tokens'][sample['valid_indices']]))\n",
    "print('_'*25)\n",
    "print('Tokens to Probe',sample['probe_indices'].shape)\n",
    "print(sample['probe_indices'])\n",
    "print([tokenizer.decode(sample['tokens'][t]) for t in sample['probe_indices']])\n",
    "print('_'*25)\n",
    "print(sample['distribution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfcd990a-109f-400e-bf37-89e5083a1760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n",
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    sample = data[i]\n",
    "    print(sample['distribution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae85518-86a7-48ce-8b12-cd39bb818531",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compound Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b8547af-345a-4a30-8b41-49a136e73268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([16200,   369,   625,  2779,   275,  1363,   342, 36192,   313,  1093,\n",
       "          7147,   470, 10543,   268,   426,   470,    15,  2640,   481,  2091,\n",
       "          4272,   715,  1029,    14]),\n",
       " 'label': 'missing_second',\n",
       " 'feature_name': 'high-school'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peek[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70ac702c-49a9-4f53-ba76-e84116867053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 167959\n",
      "{'missing_second': 67200, 'bigram': 33559, 'missing_first': 67200}\n",
      "{'high-school': 8000, 'living-room': 7959, 'social-security': 8000, 'credit-card': 8000, 'blood-pressure': 8000, 'prime-factors': 8000, 'social-media': 8000, 'gene-expression': 8000, 'control-group': 8000, 'magnetic-field': 8000, 'cell-lines': 8000, 'trial-court': 8000, 'second-derivative': 8000, 'north-america': 8000, 'human-rights': 8000, 'side-effects': 8000, 'public-health': 8000, 'federal-government': 8000, 'third-party': 8000, 'clinical-trials': 8000, 'mental-health': 8000}\n",
      "{'feature_name', 'label', 'tokens'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[2]\n",
    "print('Length of dataset', len(data))\n",
    "unique_count = {}\n",
    "unique_feature = {}\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_count[d['label']] = unique_count.get(d['label'], 0) + 1\n",
    "    unique_feature[d['feature_name']] = unique_feature.get(d['feature_name'], 0) + 1\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_count)\n",
    "print(unique_feature)\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebb5a936-7bd6-45bd-aa79-f49a6917dd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sweat from my brow. I couldn't believe I'd actually pulled it off. I'd fooled everyone in the room\n",
      "_________________________\n",
      "missing_first\n",
      "_________________________\n",
      "living-room\n",
      "_________________________\n",
      "tensor([17052,   432,   619,  6479,    15,   309,  4571,   626,  2868,   309,\n",
      "         1871,  2686,  7320,   352,   745,    15,   309,  1871, 11213,   264,\n",
      "         4130,   275,   253,  2316])\n",
      "torch.Size([24])\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "sample = data[9658]\n",
    "print(tokenizer.decode(sample['tokens']))\n",
    "print('_'*25)\n",
    "print(sample['label'])\n",
    "print('_'*25)\n",
    "print(sample['feature_name'])\n",
    "print('_'*25)\n",
    "print(sample['tokens'])\n",
    "print(sample['tokens'].shape)\n",
    "print(list(range(len(sample['tokens']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ff79a-e284-458b-8c56-b8069af88dcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddff739f-12eb-45e9-88ba-a9f49a723869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 4486\n",
      "{'is_title|probe_indices', 'start_math|probe_classes', 'end_math|probe_classes', 'is_title|valid_indices', 'is_subscript|valid_indices', 'is_reference|valid_indices', 'start_math|probe_indices', 'is_display_math|probe_indices', 'is_superscript|probe_classes', 'is_display_math|valid_indices', 'start_math|valid_indices', 'is_abstract|valid_indices', 'is_math|valid_indices', 'end_math|valid_indices', 'is_math|probe_indices', 'is_math|probe_classes', 'is_frac|probe_indices', 'is_denominator|valid_indices', 'is_display_math|probe_classes', 'is_subscript|probe_classes', 'is_author|valid_indices', 'tokens', 'is_numerator|valid_indices', 'is_author|probe_classes', 'is_inline_math|valid_indices', 'end_math|probe_indices', 'is_superscript|probe_indices', 'is_frac|probe_classes', 'is_frac|valid_indices', 'is_title|probe_classes', 'is_abstract|probe_indices', 'is_reference|probe_classes', 'is_denominator|probe_classes', 'is_inline_math|probe_indices', 'is_subscript|probe_indices', 'is_denominator|probe_indices', 'is_numerator|probe_classes', 'is_reference|probe_indices', 'is_abstract|probe_classes', 'is_author|probe_indices', 'is_numerator|probe_indices', 'is_superscript|valid_indices', 'is_inline_math|probe_classes'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[1]\n",
    "print('Length of dataset', len(data))\n",
    "\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bba46dee-25d3-4f14-bf4d-86f20389b196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>---\n",
      "abstract: 'In many real-world applications, e.g. recommendation systems, certain items appear much more frequently than other items. However, standard embedding methods—which form the basis of many ML algorithms—allocate the same dimension to all of the items. This leads to statistical and memory inefficiencies. In this work, we propose mixed dimension embedding layers in which the dimension of a particular embedding vector can depend on the frequency of the item. This approach drastically reduces the memory requirement for the embedding, while maintaining and sometimes improving the ML performance. We show that the proposed mixed dimension layers achieve a higher accuracy, while using 8$\\times$ fewer parameters, for collaborative filtering on the MovieLens dataset. Also, they improve accuracy by 0.1% using half as many parameters or maintain baseline accuracy using 16$\\times$ fewer parameters for click-through rate prediction task on the Criteo Kaggle dataset.'\n",
      "author:\n",
      "- 'Antonio A. Ginart${\\thanks{Work done while at Facebook}^{\\,}}$'\n",
      "- Maxim Naumov\n",
      "- Dheevatsa Mudigere\n",
      "- Jiyan Yang\n",
      "- James Zou\n",
      "bibliography:\n",
      "-'references.bib'\n",
      "title: 'Mixed Dimension Embedding with Application to Memory-Efficient Recommendation Systems'\n",
      "---\n",
      "\n",
      "Introduction\n",
      "============\n",
      "\n",
      "It is difficult to overstate the impact of representation learning and embedding-based models in the present AI landscape. Embedding representations power state-of-the-art applications in diverse domains, including computer vision [@barz2019hierarchy; @vasileva2018learning], natural language processing [@chiu2016train; @Mikolov2013distributed; @liu2015topical; @shoeybi2019megatron; @akbik2018contextual; @peters2018dissecting; @liu2019roberta], computational biology [@Asgari2015], and recommendation systems [@cheng2016wide; @Park2018; @Wu2019].\n",
      "\n",
      "There seems to be a fundamental trade-off between the dimension of an embedding representation and the statistical performance (i.e. accuracy or loss) of embedding-based models on a particular learning task. It is well-documented that statistical performance suffers when embedding dimension is too low [@yin2018dimensionality]. Thus, we are interested in the fundamental question: *Is it possible to re-architect embedding representations for a more favorable trade-off between number of parameters and statistical performance?*\n",
      "\n",
      "This challenge is particularly prominent in recommendation systems tasks such as collaborative filtering (CF) and click-through rate (CTR) prediction problems. Recommendation models power some of the most commonplace data-driven services on the internet that benefit users across the globe with personalized experiences on a daily basis [@Aggarwal2016; @hazelwood2018; @he2014practical; @Pi2019].\n",
      "\n",
      "At present, the out-sized memory consumption of standard embedding layers in these recommendation systems is a major burden on the memory hierarchy – an embedding layer for a single recommendation engine can consume tens of gigabytes of space [@Park2018; @Pi2019]. This makes the engineering challenges associated with large-scale recommendation embeddings of particular importance and interest.\n",
      "\n",
      "In many models, embedding layers are used to map input categorical values into a vector space. This feature mapping outputs a $d$-dimensional vector for each value, and is learned concurrently with the model during supervised training. However, the relationship between data distributions, embedding representations, model architectures, and optimization algorithms is highly complex. This makes it difficult to design efficient quantization and compression algorithms for embedding representations from information-theoretic principles.\n",
      "\n",
      "Nevertheless, the distributions of accesses for users and items is often heavily skewed in many real-world applications. For instance, for the CF task on MovieLens dataset[^1], the top 10% of users receive as many queries as the remaining 90% and the top 1% of items receive as many queries as the remaining 99%. To an even greater extent, for the CTR prediction task on the Criteo AI Labs Ad Kaggle dataset[^2] the top $0.0003\\%$ of indices receive as many queries as the remaining 30 million, as summarized on Fig. \\[fig:hist\\_datasets\\]. In networks science, this phenomena is referred to as *popularity* [@cho2012wave; @papadopoulos2012popularity], a terminology we adopt here.\n",
      "\n",
      "[0.32]{}![Histogram of accesses for different embedding vectors[]{data-label=\"fig:hist_datasets\"}](images/hist_ml_items.png \"fig:\"){width=\"\\textwidth\"}\n",
      "\n",
      "[0.32]{}![Histogram of accesses for different embedding vectors[]{data-label=\"fig:hist_datasets\"}](images/hist_ml_user.png \"fig:\"){width=\"\\textwidth\"}\n",
      "\n",
      "[0.32]{}![Hist\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "sample = data[106]\n",
    "print(tokenizer.decode(sample['tokens']))\n",
    "print('_'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89eb0918-3a4d-4634-bf80-bad546cc4a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_keys) == len(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e3456fa-8170-4772-bee6-d02fe56d3e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: torch.Size([1024])\n",
      "_________________________\n",
      "1) is_title|valid_indices <> is_title|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([5])\n",
      "tensor([19013,  3836,   275,   308,  4063, 17827,    27, 19540, 26253,   980,\n",
      "        24704])\n",
      "tensor([32125,  4063, 19540, 24704,  7884])\n",
      "_________________________\n",
      "2) is_subscript|valid_indices <> is_subscript|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  7884])\n",
      "_________________________\n",
      "3) is_reference|valid_indices <> is_reference|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([7])\n",
      "tensor([   39,  4448,   348,    36, 12456,    28,  1214, 27415,  2610,   589,\n",
      "         7857,    28,  1214, 10859,   360, 12844,  6755,    28,  1214,    39,\n",
      "         4448,   348,  6755,    28,  1214,  9263,  4396,   280, 28205,  6961,\n",
      "           28,  1214,    45,   266, 32567,  6961,    59, 11801,  9755,    28,\n",
      "         1214, 23262,  7857,    28,  1214, 10859,   360, 12844,  7199,    28,\n",
      "         1214, 25302,    91, 41717,  7132,    40,   780,    66,  8602,    28,\n",
      "         1214, 22176, 25185,  8012,    28,  1214, 23262,  6620,    28,  1214,\n",
      "        21892, 14617,  1441,  6620,    28,  1214, 24441,  1665,  6620,    28,\n",
      "         1214, 23262,  7798,    45,   739, 17146,  6622,  3594,   357, 15767,\n",
      "         6622,    28,  1214, 32031,  7798,    49,   360,   343,  6961,    84,\n",
      "         1504, 29982, 12430,    28,  1214,    35,   886,  6961,    28,  1214,\n",
      "         2374, 31384,  6620,    87,   706,    76,  6968,   282,  6759,    28,\n",
      "         1214,    49,   360,   343,  6961,    28,  1214,    42, 10443,  6961,\n",
      "           35,   886,  6961])\n",
      "tensor([32125,   527,    28,  4448,  7199,   343,  7884])\n",
      "_________________________\n",
      "4) is_display_math|valid_indices <> is_display_math|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([3])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  9929,  7884])\n",
      "_________________________\n",
      "5) start_math|valid_indices <> start_math|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([3])\n",
      "tensor([ 2367, 49938,  9929,   669,   669,   310])\n",
      "tensor([32125,  9929,  7884])\n",
      "_________________________\n",
      "6) is_abstract|valid_indices <> is_abstract|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([4])\n",
      "tensor([  686,   510,  2554,   273,   253,  8820,  2605,   273,   247, 33217,\n",
      "         2685,   275, 22474,  8091, 15708,  4142,   275, 39174,   310,   271,\n",
      "         1527,  1953,    15,   844,   921,   285, 22048,    13,   347,   247,\n",
      "         1159,   273,  8091, 41299,    13,   253,  5921,   875,   253,  1554,\n",
      "         2865,  1079,  5289,   273, 32125])\n",
      "tensor([ 1554, 32125,   527,  7884])\n",
      "_________________________\n",
      "7) is_math|valid_indices <> is_math|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([4])\n",
      "tensor([ 2367,  1588,  1165, 49938,    22,    15,    23,   393,  2609,   464,\n",
      "         4841,  1926,  1588,  1165,    63,    19,   889,  3799,  9929,    19,\n",
      "          393,  2609,   464,  4841,  1926,  1588,  1165,    63,    19,   889,\n",
      "         3799,   669,  5844,   669,  6526,    61,  3830,  1926,  5844,  1484,\n",
      "           94,   426])\n",
      "tensor([32125,   527,  7199,  7884])\n",
      "_________________________\n",
      "8) end_math|valid_indices <> end_math|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([3])\n",
      "tensor([  724, 17042, 34942,     5,  7884,   310])\n",
      "tensor([32125,  9929,  7884])\n",
      "_________________________\n",
      "9) is_denominator|valid_indices <> is_denominator|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  7884])\n",
      "_________________________\n",
      "10) is_author|valid_indices <> is_author|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([5])\n",
      "tensor([  187,    14,   686, 41237,   416,    15, 20424, 15916,     8,   187,\n",
      "           14, 45802,  1240,   334,  3419,  7110,   394,  5973,   187,    14,\n",
      "          416,  2902, 21806,   527,   274, 48964,   187,    14,  5769,    83,\n",
      "         2016,  5801,   322,  1164,   274, 10734,   187])\n",
      "tensor([32125, 45802,   527,  5769,  7884])\n",
      "_________________________\n",
      "11) is_numerator|valid_indices <> is_numerator|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  7884])\n",
      "_________________________\n",
      "12) is_inline_math|valid_indices <> is_inline_math|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([ 2367,  1588,  1165, 49938,    22,    15,    23,   393,  2609,   464,\n",
      "         4841,  1926,  1588,  1165,    63,    19,   889,  3799,  9929,    19,\n",
      "          393,  2609,   464,  4841,  1926,  1588,  1165,    63,    19,   889,\n",
      "         3799,   669,  5844,   669,  6526,    61,  3830,  1926,  5844,  1484,\n",
      "           94,   426])\n",
      "tensor([32125,  7884])\n",
      "_________________________\n",
      "13) is_frac|valid_indices <> is_frac|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  7884])\n",
      "_________________________\n",
      "14) is_superscript|valid_indices <> is_superscript|probe_indices\n",
      "Valid Shape torch.Size([1024])\n",
      "Probe Shape torch.Size([2])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([32125,  7884])\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sample = data[0]\n",
    "\n",
    "print('Tokens:',sample['tokens'].shape)\n",
    "print('_'*25)\n",
    "c = 1\n",
    "for k in unique_keys:\n",
    "    if 'valid_indices' in k:\n",
    "        pr_key = re.sub('valid_indices','probe_indices',k)\n",
    "        if pr_key not in sample:\n",
    "            pr_key = 'N/A'\n",
    "        print(f\"{c}) {k} <> {pr_key}\")\n",
    "        print('Valid Shape',sample[k].shape)\n",
    "        print('Probe Shape',sample[pr_key].shape)\n",
    "        #print(sample[k][sample[pr_key]])\n",
    "        valid_tokens = sample['tokens'][sample[k]]\n",
    "        print(valid_tokens)\n",
    "        probe_tokens = sample['tokens'][sample[pr_key]]\n",
    "        print(probe_tokens)\n",
    "        print('_'*25)\n",
    "        c+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bac82647-6350-471f-894d-d546da7f1318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample['is_title|probe_indices'], sample['start_math|probe_indices'], sample['is_display_math|probe_indices'], sample['is_math|probe_indices'], sample['is_frac|probe_indices'], sample['end_math|probe_indices'], sample['is_superscript|probe_indices'], sample['is_abstract|probe_indices'], sample['is_inline_math|probe_indices'], sample['is_subscript|probe_indices'], sample['is_denominator|probe_indices'], sample['is_reference|probe_indices'], sample['is_author|probe_indices'], sample['is_numerator|probe_indices'], "
     ]
    }
   ],
   "source": [
    "for k in unique_keys:\n",
    "    if 'probe_indices' in k:\n",
    "        print(f\"sample['{k}'],\",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4fa3739-936f-49e5-9cff-463fcbfa582a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 140, 44, 237, 175, 589, 49, 178, 339, 145, 181, 952, 252, 1022]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[0]\n",
    "\n",
    "list(set(torch.cat([sample['is_title|probe_indices'], sample['start_math|probe_indices'], sample['is_display_math|probe_indices'], sample['is_math|probe_indices'], sample['is_frac|probe_indices'], sample['end_math|probe_indices'], sample['is_superscript|probe_indices'], sample['is_abstract|probe_indices'], sample['is_inline_math|probe_indices'], sample['is_subscript|probe_indices'], sample['is_denominator|probe_indices'], sample['is_reference|probe_indices'], sample['is_author|probe_indices'], sample['is_numerator|probe_indices']]).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256fb62-cdb7-4233-a454-950a19407839",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3dd54d26-3ac7-4300-a62a-3f52440afe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset 1438\n",
      "{'dep_root|probe_indices', 'Person_2|probe_classes', 'eos_True|probe_classes', 'Gender_Masc|probe_classes', 'upos_PROPN|probe_indices', 'NumType', 'upos_SCONJ|probe_classes', 'dep_ccomp|probe_classes', 'VerbForm_Ger|probe_indices', 'dep_obj|probe_classes', 'Mood_Imp|probe_indices', 'Mood', 'upos_VERB|probe_classes', 'Gender_Fem|probe_indices', 'Gender_Fem|probe_classes', 'dep_conj|probe_indices', 'PronType_Rel|probe_indices', 'upos_NUM|probe_classes', 'upos_DET|probe_classes', 'Person_3|probe_classes', 'Voice', 'doc_id', 'dep_advmod|probe_classes', 'dep_obj|probe_indices', 'VerbForm_Fin|probe_indices', 'dep_nsubj:pass|probe_indices', 'PronType', 'upos_SYM|probe_indices', 'upos_PUNC|probe_indices', 'dep_parataxis|probe_classes', 'PronType_Prs|probe_classes', 'VerbForm_Ger|probe_classes', 'PronType_Int|probe_indices', 'dep_parataxis|probe_indices', 'tokens', 'upos_ADJ|probe_indices', 'dep_nsubj|probe_classes', 'VerbForm_Inf|probe_classes', 'upos_SYM|probe_classes', 'Number_Plur|probe_indices', 'upos_ADV|probe_classes', 'Voice_Pass|probe_classes', 'upos_CCONJ|probe_classes', 'dep_advmod|probe_indices', 'dep_case|probe_indices', 'upos_NOUN|probe_classes', 'Tense_Past|probe_indices', 'dep_acl|probe_classes', 'dep_aux|probe_indices', 'dep_xcomp|probe_classes', 'dep_list|probe_classes', 'PronType_Prs|probe_indices', 'PronType_Int|probe_classes', 'Gender_Neut|probe_indices', 'dep_amod|probe_classes', 'PronType_Dem|probe_classes', 'eos', 'upos_PROPN|probe_classes', 'within_compound_token_ix', 'dep_root|probe_classes', 'first_eos_True|probe_indices', 'upos_AUX|probe_indices', 'Person_1|probe_indices', 'dep_nmod|probe_classes', 'dep_flat|probe_indices', 'dep_nummod|probe_classes', 'VerbForm_Part|probe_classes', 'dep_advcl|probe_indices', 'dep_nmod:poss|probe_classes', 'NumType_Card|probe_indices', 'first_eos_True|probe_classes', 'upos_NUM|probe_indices', 'upos_ADV|probe_indices', 'dep_aux|probe_classes', 'PronType_Rel|probe_classes', 'split', 'dep_acl:relcl|probe_indices', 'dep_acl:relcl|probe_classes', 'Mood_Imp|probe_classes', 'dep_conj|probe_classes', 'Number', 'dep_cop|probe_classes', 'dep_nmod|probe_indices', 'first_eos', 'position', 'upos_X|probe_classes', 'upos', 'PronType_Art|probe_classes', 'max_compound_token_ix', 'Person_1|probe_classes', 'Tense_Past|probe_classes', 'upos_DET|probe_indices', 'VerbForm_Fin|probe_classes', 'VerbForm_Part|probe_indices', 'Voice_Pass|probe_indices', 'dep_obl|probe_indices', 'upos_VERB|probe_indices', 'upos_ADP|probe_classes', 'dep_xcomp|probe_indices', 'PronType_Art|probe_indices', 'upos_ADJ|probe_classes', 'dep_cc|probe_classes', 'dep_aux:pass|probe_classes', 'dep_case|probe_classes', 'dep_det|probe_classes', 'dep_mark|probe_classes', 'dep_nummod|probe_indices', 'upos_ADP|probe_indices', 'Person_2|probe_indices', 'upos_SCONJ|probe_indices', 'dep_det|probe_indices', 'PronType_Dem|probe_indices', 'dep_cc|probe_indices', 'upos_PUNC|probe_classes', 'dep_advcl|probe_classes', 'dep_compound|probe_indices', 'dep_punct|probe_classes', 'upos_X|probe_indices', 'upos_PRON|probe_classes', 'dep_compound|probe_classes', 'upos_INTJ|probe_classes', 'dep_obl|probe_classes', 'upos_NOUN|probe_indices', 'dep_appos|probe_indices', 'dep_flat|probe_classes', 'dep_appos|probe_classes', 'dep_cop|probe_indices', 'dep', 'dep_nsubj:pass|probe_classes', 'dep_list|probe_indices', 'Number_Plur|probe_classes', 'upos_INTJ|probe_indices', 'upos_AUX|probe_classes', 'upos_CCONJ|probe_indices', 'upos_PRON|probe_indices', 'dep_aux:pass|probe_indices', 'Person_3|probe_indices', 'Gender', 'dep_nsubj|probe_indices', 'eos_True|probe_indices', 'NumType_Card|probe_classes', 'VerbForm_Inf|probe_indices', 'head', 'dep_acl|probe_indices', 'Tense', 'dep_mark|probe_indices', 'Person', 'Gender_Masc|probe_indices', 'VerbForm', 'dep_amod|probe_indices', 'dep_ccomp|probe_indices', 'dep_punct|probe_indices', 'dep_nmod:poss|probe_indices', 'Gender_Neut|probe_classes'}\n"
     ]
    }
   ],
   "source": [
    "data = peek[0]\n",
    "print('Length of dataset', len(data))\n",
    "\n",
    "unique_keys = set()\n",
    "for d in data:\n",
    "    unique_keys.update(d.keys())\n",
    "\n",
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f1749a40-39ed-4d33-989f-5d4cb2df64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample['dep_root|probe_indices'], sample['upos_PROPN|probe_indices'], sample['VerbForm_Ger|probe_indices'], sample['Mood_Imp|probe_indices'], sample['Gender_Fem|probe_indices'], sample['dep_conj|probe_indices'], sample['PronType_Rel|probe_indices'], sample['dep_obj|probe_indices'], sample['VerbForm_Fin|probe_indices'], sample['dep_nsubj:pass|probe_indices'], sample['upos_SYM|probe_indices'], sample['upos_PUNC|probe_indices'], sample['PronType_Int|probe_indices'], sample['dep_parataxis|probe_indices'], sample['upos_ADJ|probe_indices'], sample['Number_Plur|probe_indices'], sample['dep_advmod|probe_indices'], sample['dep_case|probe_indices'], sample['Tense_Past|probe_indices'], sample['dep_aux|probe_indices'], sample['PronType_Prs|probe_indices'], sample['Gender_Neut|probe_indices'], sample['first_eos_True|probe_indices'], sample['upos_AUX|probe_indices'], sample['Person_1|probe_indices'], sample['dep_flat|probe_indices'], sample['dep_advcl|probe_indices'], sample['NumType_Card|probe_indices'], sample['upos_NUM|probe_indices'], sample['upos_ADV|probe_indices'], sample['dep_acl:relcl|probe_indices'], sample['dep_nmod|probe_indices'], sample['upos_DET|probe_indices'], sample['VerbForm_Part|probe_indices'], sample['Voice_Pass|probe_indices'], sample['dep_obl|probe_indices'], sample['upos_VERB|probe_indices'], sample['dep_xcomp|probe_indices'], sample['PronType_Art|probe_indices'], sample['dep_nummod|probe_indices'], sample['upos_ADP|probe_indices'], sample['Person_2|probe_indices'], sample['upos_SCONJ|probe_indices'], sample['dep_det|probe_indices'], sample['PronType_Dem|probe_indices'], sample['dep_cc|probe_indices'], sample['dep_compound|probe_indices'], sample['upos_X|probe_indices'], sample['upos_NOUN|probe_indices'], sample['dep_appos|probe_indices'], sample['dep_cop|probe_indices'], sample['dep_list|probe_indices'], sample['upos_INTJ|probe_indices'], sample['upos_CCONJ|probe_indices'], sample['upos_PRON|probe_indices'], sample['dep_aux:pass|probe_indices'], sample['Person_3|probe_indices'], sample['dep_nsubj|probe_indices'], sample['eos_True|probe_indices'], sample['VerbForm_Inf|probe_indices'], sample['dep_acl|probe_indices'], sample['dep_mark|probe_indices'], sample['Gender_Masc|probe_indices'], sample['dep_amod|probe_indices'], sample['dep_ccomp|probe_indices'], sample['dep_punct|probe_indices'], sample['dep_nmod:poss|probe_indices'], "
     ]
    }
   ],
   "source": [
    "sample = data[0]\n",
    "for k in unique_keys:\n",
    "    if 'probe_indices' in k:\n",
    "        print(f\"sample['{k}'],\",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b691d72f-353f-4479-aa0b-2759e5e55b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dep_root|probe_indices:torch.Size([40])\n",
      "2 Person_2|probe_classes:torch.Size([27])\n",
      "3 eos_True|probe_classes:torch.Size([40])\n",
      "4 Gender_Masc|probe_classes:torch.Size([7])\n",
      "5 upos_PROPN|probe_indices:torch.Size([56])\n",
      "6 NumType:torch.Size([512])\n",
      "7 upos_SCONJ|probe_classes:torch.Size([45])\n",
      "8 dep_ccomp|probe_classes:torch.Size([42])\n",
      "9 VerbForm_Ger|probe_indices:torch.Size([45])\n",
      "10 dep_obj|probe_classes:torch.Size([40])\n",
      "11 Mood_Imp|probe_indices:torch.Size([31])\n",
      "12 Mood:torch.Size([512])\n",
      "13 upos_VERB|probe_classes:torch.Size([40])\n",
      "14 Gender_Fem|probe_indices:torch.Size([7])\n",
      "15 Gender_Fem|probe_classes:torch.Size([7])\n",
      "16 dep_conj|probe_indices:torch.Size([39])\n",
      "17 PronType_Rel|probe_indices:torch.Size([47])\n",
      "18 upos_NUM|probe_classes:torch.Size([38])\n",
      "19 upos_DET|probe_classes:torch.Size([48])\n",
      "20 Person_3|probe_classes:torch.Size([15])\n",
      "21 Voice:torch.Size([512])\n",
      "22 doc_id:weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\n",
      "23 dep_advmod|probe_classes:torch.Size([39])\n",
      "24 dep_obj|probe_indices:torch.Size([40])\n",
      "25 VerbForm_Fin|probe_indices:torch.Size([45])\n",
      "26 dep_nsubj:pass|probe_indices:torch.Size([48])\n",
      "27 PronType:torch.Size([512])\n",
      "28 upos_SYM|probe_indices:torch.Size([39])\n",
      "29 upos_PUNC|probe_indices:torch.Size([38])\n",
      "30 dep_parataxis|probe_classes:torch.Size([42])\n",
      "31 PronType_Prs|probe_classes:torch.Size([53])\n",
      "32 VerbForm_Ger|probe_classes:torch.Size([45])\n",
      "33 PronType_Int|probe_indices:torch.Size([45])\n",
      "34 dep_parataxis|probe_indices:torch.Size([42])\n",
      "35 tokens:torch.Size([512])\n",
      "36 upos_ADJ|probe_indices:torch.Size([38])\n",
      "37 dep_nsubj|probe_classes:torch.Size([44])\n",
      "38 VerbForm_Inf|probe_classes:torch.Size([42])\n",
      "39 upos_SYM|probe_classes:torch.Size([39])\n",
      "40 Number_Plur|probe_indices:torch.Size([65])\n",
      "41 upos_ADV|probe_classes:torch.Size([38])\n",
      "42 Voice_Pass|probe_classes:torch.Size([35])\n",
      "43 upos_CCONJ|probe_classes:torch.Size([38])\n",
      "44 dep_advmod|probe_indices:torch.Size([39])\n",
      "45 dep_case|probe_indices:torch.Size([45])\n",
      "46 upos_NOUN|probe_classes:torch.Size([41])\n",
      "47 Tense_Past|probe_indices:torch.Size([32])\n",
      "48 dep_acl|probe_classes:torch.Size([42])\n",
      "49 dep_aux|probe_indices:torch.Size([41])\n",
      "50 dep_xcomp|probe_classes:torch.Size([40])\n",
      "51 dep_list|probe_classes:torch.Size([38])\n",
      "52 PronType_Prs|probe_indices:torch.Size([53])\n",
      "53 PronType_Int|probe_classes:torch.Size([45])\n",
      "54 Gender_Neut|probe_indices:torch.Size([7])\n",
      "55 dep_amod|probe_classes:torch.Size([44])\n",
      "56 PronType_Dem|probe_classes:torch.Size([45])\n",
      "57 eos:torch.Size([512])\n",
      "58 upos_PROPN|probe_classes:torch.Size([56])\n",
      "59 within_compound_token_ix:torch.Size([512])\n",
      "60 dep_root|probe_classes:torch.Size([40])\n",
      "61 first_eos_True|probe_indices:torch.Size([21])\n",
      "62 upos_AUX|probe_indices:torch.Size([40])\n",
      "63 Person_1|probe_indices:torch.Size([26])\n",
      "64 dep_nmod|probe_classes:torch.Size([52])\n",
      "65 dep_flat|probe_indices:torch.Size([53])\n",
      "66 dep_nummod|probe_classes:torch.Size([42])\n",
      "67 VerbForm_Part|probe_classes:torch.Size([44])\n",
      "68 dep_advcl|probe_indices:torch.Size([41])\n",
      "69 dep_nmod:poss|probe_classes:torch.Size([40])\n",
      "70 NumType_Card|probe_indices:torch.Size([38])\n",
      "71 first_eos_True|probe_classes:torch.Size([21])\n",
      "72 upos_NUM|probe_indices:torch.Size([38])\n",
      "73 upos_ADV|probe_indices:torch.Size([38])\n",
      "74 dep_aux|probe_classes:torch.Size([41])\n",
      "75 PronType_Rel|probe_classes:torch.Size([47])\n",
      "76 split:train\n",
      "77 dep_acl:relcl|probe_indices:torch.Size([39])\n",
      "78 dep_acl:relcl|probe_classes:torch.Size([39])\n",
      "79 Mood_Imp|probe_classes:torch.Size([31])\n",
      "80 dep_conj|probe_classes:torch.Size([39])\n",
      "81 Number:torch.Size([512])\n",
      "82 dep_cop|probe_classes:torch.Size([37])\n",
      "83 dep_nmod|probe_indices:torch.Size([52])\n",
      "84 first_eos:torch.Size([512])\n",
      "85 position:torch.Size([512])\n",
      "86 upos_X|probe_classes:torch.Size([35])\n",
      "87 upos:torch.Size([512])\n",
      "88 PronType_Art|probe_classes:torch.Size([35])\n",
      "89 max_compound_token_ix:torch.Size([512])\n",
      "90 Person_1|probe_classes:torch.Size([26])\n",
      "91 Tense_Past|probe_classes:torch.Size([32])\n",
      "92 upos_DET|probe_indices:torch.Size([48])\n",
      "93 VerbForm_Fin|probe_classes:torch.Size([45])\n",
      "94 VerbForm_Part|probe_indices:torch.Size([44])\n",
      "95 Voice_Pass|probe_indices:torch.Size([35])\n",
      "96 dep_obl|probe_indices:torch.Size([42])\n",
      "97 upos_VERB|probe_indices:torch.Size([40])\n",
      "98 upos_ADP|probe_classes:torch.Size([44])\n",
      "99 dep_xcomp|probe_indices:torch.Size([40])\n",
      "100 PronType_Art|probe_indices:torch.Size([35])\n",
      "101 upos_ADJ|probe_classes:torch.Size([38])\n",
      "102 dep_cc|probe_classes:torch.Size([38])\n",
      "103 dep_aux:pass|probe_classes:torch.Size([46])\n",
      "104 dep_case|probe_classes:torch.Size([45])\n",
      "105 dep_det|probe_classes:torch.Size([49])\n",
      "106 dep_mark|probe_classes:torch.Size([41])\n",
      "107 dep_nummod|probe_indices:torch.Size([42])\n",
      "108 upos_ADP|probe_indices:torch.Size([44])\n",
      "109 Person_2|probe_indices:torch.Size([27])\n",
      "110 upos_SCONJ|probe_indices:torch.Size([45])\n",
      "111 dep_det|probe_indices:torch.Size([49])\n",
      "112 PronType_Dem|probe_indices:torch.Size([45])\n",
      "113 dep_cc|probe_indices:torch.Size([38])\n",
      "114 upos_PUNC|probe_classes:torch.Size([38])\n",
      "115 dep_advcl|probe_classes:torch.Size([41])\n",
      "116 dep_compound|probe_indices:torch.Size([52])\n",
      "117 dep_punct|probe_classes:torch.Size([38])\n",
      "118 upos_X|probe_indices:torch.Size([35])\n",
      "119 upos_PRON|probe_classes:torch.Size([40])\n",
      "120 dep_compound|probe_classes:torch.Size([52])\n",
      "121 upos_INTJ|probe_classes:torch.Size([39])\n",
      "122 dep_obl|probe_classes:torch.Size([42])\n",
      "123 upos_NOUN|probe_indices:torch.Size([41])\n",
      "124 dep_appos|probe_indices:torch.Size([42])\n",
      "125 dep_flat|probe_classes:torch.Size([53])\n",
      "126 dep_appos|probe_classes:torch.Size([42])\n",
      "127 dep_cop|probe_indices:torch.Size([37])\n",
      "128 dep:torch.Size([512])\n",
      "129 dep_nsubj:pass|probe_classes:torch.Size([48])\n",
      "130 dep_list|probe_indices:torch.Size([38])\n",
      "131 Number_Plur|probe_classes:torch.Size([65])\n",
      "132 upos_INTJ|probe_indices:torch.Size([39])\n",
      "133 upos_AUX|probe_classes:torch.Size([40])\n",
      "134 upos_CCONJ|probe_indices:torch.Size([38])\n",
      "135 upos_PRON|probe_indices:torch.Size([40])\n",
      "136 dep_aux:pass|probe_indices:torch.Size([46])\n",
      "137 Person_3|probe_indices:torch.Size([15])\n",
      "138 Gender:torch.Size([512])\n",
      "139 dep_nsubj|probe_indices:torch.Size([44])\n",
      "140 eos_True|probe_indices:torch.Size([40])\n",
      "141 NumType_Card|probe_classes:torch.Size([38])\n",
      "142 VerbForm_Inf|probe_indices:torch.Size([42])\n",
      "143 head:torch.Size([512])\n",
      "144 dep_acl|probe_indices:torch.Size([42])\n",
      "145 Tense:torch.Size([512])\n",
      "146 dep_mark|probe_indices:torch.Size([41])\n",
      "147 Person:torch.Size([512])\n",
      "148 Gender_Masc|probe_indices:torch.Size([7])\n",
      "149 VerbForm:torch.Size([512])\n",
      "150 dep_amod|probe_indices:torch.Size([44])\n",
      "151 dep_ccomp|probe_indices:torch.Size([42])\n",
      "152 dep_punct|probe_indices:torch.Size([38])\n",
      "153 dep_nmod:poss|probe_indices:torch.Size([40])\n",
      "154 Gender_Neut|probe_classes:torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "upos = []\n",
    "dep = []\n",
    "other = []\n",
    "for i, k in enumerate(unique_keys):\n",
    "    print(i+1,end=' ')\n",
    "    if isinstance(sample[k], torch.Tensor):\n",
    "        print(f'{k}:{sample[k].shape}')\n",
    "    else:\n",
    "        print(f'{k}:{sample[k]}')\n",
    "    if 'upos' in k:\n",
    "        upos.append(k)\n",
    "    elif 'dep' in k:\n",
    "        dep.append(k)\n",
    "    else:\n",
    "        other.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6fffe07-d044-4ccc-9818-d3f58a92a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 59, 62)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(upos), len(dep), len(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "253a95f7-80d1-4228-bd0c-3cf0e988fe42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2add21b3-7e6d-414f-9b54-62487fcec1b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upos_PROPN|probe_indices',\n",
       " 'upos_SCONJ|probe_classes',\n",
       " 'upos_VERB|probe_classes',\n",
       " 'upos_NUM|probe_classes',\n",
       " 'upos_DET|probe_classes',\n",
       " 'upos_SYM|probe_indices',\n",
       " 'upos_PUNC|probe_indices',\n",
       " 'upos_ADJ|probe_indices',\n",
       " 'upos_SYM|probe_classes',\n",
       " 'upos_ADV|probe_classes',\n",
       " 'upos_CCONJ|probe_classes',\n",
       " 'upos_NOUN|probe_classes',\n",
       " 'upos_PROPN|probe_classes',\n",
       " 'upos_AUX|probe_indices',\n",
       " 'upos_NUM|probe_indices',\n",
       " 'upos_ADV|probe_indices',\n",
       " 'upos_X|probe_classes',\n",
       " 'upos',\n",
       " 'upos_DET|probe_indices',\n",
       " 'upos_VERB|probe_indices',\n",
       " 'upos_ADP|probe_classes',\n",
       " 'upos_ADJ|probe_classes',\n",
       " 'upos_ADP|probe_indices',\n",
       " 'upos_SCONJ|probe_indices',\n",
       " 'upos_PUNC|probe_classes',\n",
       " 'upos_X|probe_indices',\n",
       " 'upos_PRON|probe_classes',\n",
       " 'upos_INTJ|probe_classes',\n",
       " 'upos_NOUN|probe_indices',\n",
       " 'upos_INTJ|probe_indices',\n",
       " 'upos_AUX|probe_classes',\n",
       " 'upos_CCONJ|probe_indices',\n",
       " 'upos_PRON|probe_indices']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ddbf6e75-a6d3-4249-a730-13f77725ba74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dep_root|probe_indices',\n",
       " 'dep_ccomp|probe_classes',\n",
       " 'dep_obj|probe_classes',\n",
       " 'dep_conj|probe_indices',\n",
       " 'dep_advmod|probe_classes',\n",
       " 'dep_obj|probe_indices',\n",
       " 'dep_nsubj:pass|probe_indices',\n",
       " 'dep_parataxis|probe_classes',\n",
       " 'dep_parataxis|probe_indices',\n",
       " 'dep_nsubj|probe_classes',\n",
       " 'dep_advmod|probe_indices',\n",
       " 'dep_case|probe_indices',\n",
       " 'dep_acl|probe_classes',\n",
       " 'dep_aux|probe_indices',\n",
       " 'dep_xcomp|probe_classes',\n",
       " 'dep_list|probe_classes',\n",
       " 'dep_amod|probe_classes',\n",
       " 'dep_root|probe_classes',\n",
       " 'dep_nmod|probe_classes',\n",
       " 'dep_flat|probe_indices',\n",
       " 'dep_nummod|probe_classes',\n",
       " 'dep_advcl|probe_indices',\n",
       " 'dep_nmod:poss|probe_classes',\n",
       " 'dep_aux|probe_classes',\n",
       " 'dep_acl:relcl|probe_indices',\n",
       " 'dep_acl:relcl|probe_classes',\n",
       " 'dep_conj|probe_classes',\n",
       " 'dep_cop|probe_classes',\n",
       " 'dep_nmod|probe_indices',\n",
       " 'dep_obl|probe_indices',\n",
       " 'dep_xcomp|probe_indices',\n",
       " 'dep_cc|probe_classes',\n",
       " 'dep_aux:pass|probe_classes',\n",
       " 'dep_case|probe_classes',\n",
       " 'dep_det|probe_classes',\n",
       " 'dep_mark|probe_classes',\n",
       " 'dep_nummod|probe_indices',\n",
       " 'dep_det|probe_indices',\n",
       " 'dep_cc|probe_indices',\n",
       " 'dep_advcl|probe_classes',\n",
       " 'dep_compound|probe_indices',\n",
       " 'dep_punct|probe_classes',\n",
       " 'dep_compound|probe_classes',\n",
       " 'dep_obl|probe_classes',\n",
       " 'dep_appos|probe_indices',\n",
       " 'dep_flat|probe_classes',\n",
       " 'dep_appos|probe_classes',\n",
       " 'dep_cop|probe_indices',\n",
       " 'dep',\n",
       " 'dep_nsubj:pass|probe_classes',\n",
       " 'dep_list|probe_indices',\n",
       " 'dep_aux:pass|probe_indices',\n",
       " 'dep_nsubj|probe_indices',\n",
       " 'dep_acl|probe_indices',\n",
       " 'dep_mark|probe_indices',\n",
       " 'dep_amod|probe_indices',\n",
       " 'dep_ccomp|probe_indices',\n",
       " 'dep_punct|probe_indices',\n",
       " 'dep_nmod:poss|probe_indices']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e7d0fb97-3993-4f4d-9c70-29d65b7edd26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Person_2|probe_classes': torch.Size([27]),\n",
       " 'eos_True|probe_classes': torch.Size([40]),\n",
       " 'Gender_Masc|probe_classes': torch.Size([7]),\n",
       " 'NumType': torch.Size([512]),\n",
       " 'VerbForm_Ger|probe_indices': torch.Size([45]),\n",
       " 'Mood_Imp|probe_indices': torch.Size([31]),\n",
       " 'Mood': torch.Size([512]),\n",
       " 'Gender_Fem|probe_indices': torch.Size([7]),\n",
       " 'Gender_Fem|probe_classes': torch.Size([7]),\n",
       " 'PronType_Rel|probe_indices': torch.Size([47]),\n",
       " 'Person_3|probe_classes': torch.Size([15]),\n",
       " 'Voice': torch.Size([512]),\n",
       " 'doc_id': 'weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000',\n",
       " 'VerbForm_Fin|probe_indices': torch.Size([45]),\n",
       " 'PronType': torch.Size([512]),\n",
       " 'PronType_Prs|probe_classes': torch.Size([53]),\n",
       " 'VerbForm_Ger|probe_classes': torch.Size([45]),\n",
       " 'PronType_Int|probe_indices': torch.Size([45]),\n",
       " 'tokens': torch.Size([512]),\n",
       " 'VerbForm_Inf|probe_classes': torch.Size([42]),\n",
       " 'Number_Plur|probe_indices': torch.Size([65]),\n",
       " 'Voice_Pass|probe_classes': torch.Size([35]),\n",
       " 'Tense_Past|probe_indices': torch.Size([32]),\n",
       " 'PronType_Prs|probe_indices': torch.Size([53]),\n",
       " 'PronType_Int|probe_classes': torch.Size([45]),\n",
       " 'Gender_Neut|probe_indices': torch.Size([7]),\n",
       " 'PronType_Dem|probe_classes': torch.Size([45]),\n",
       " 'eos': torch.Size([512]),\n",
       " 'within_compound_token_ix': torch.Size([512]),\n",
       " 'first_eos_True|probe_indices': torch.Size([21]),\n",
       " 'Person_1|probe_indices': torch.Size([26]),\n",
       " 'VerbForm_Part|probe_classes': torch.Size([44]),\n",
       " 'NumType_Card|probe_indices': torch.Size([38]),\n",
       " 'first_eos_True|probe_classes': torch.Size([21]),\n",
       " 'PronType_Rel|probe_classes': torch.Size([47]),\n",
       " 'split': 'train',\n",
       " 'Mood_Imp|probe_classes': torch.Size([31]),\n",
       " 'Number': torch.Size([512]),\n",
       " 'first_eos': torch.Size([512]),\n",
       " 'position': torch.Size([512]),\n",
       " 'PronType_Art|probe_classes': torch.Size([35]),\n",
       " 'max_compound_token_ix': torch.Size([512]),\n",
       " 'Person_1|probe_classes': torch.Size([26]),\n",
       " 'Tense_Past|probe_classes': torch.Size([32]),\n",
       " 'VerbForm_Fin|probe_classes': torch.Size([45]),\n",
       " 'VerbForm_Part|probe_indices': torch.Size([44]),\n",
       " 'Voice_Pass|probe_indices': torch.Size([35]),\n",
       " 'PronType_Art|probe_indices': torch.Size([35]),\n",
       " 'Person_2|probe_indices': torch.Size([27]),\n",
       " 'PronType_Dem|probe_indices': torch.Size([45]),\n",
       " 'Number_Plur|probe_classes': torch.Size([65]),\n",
       " 'Person_3|probe_indices': torch.Size([15]),\n",
       " 'Gender': torch.Size([512]),\n",
       " 'eos_True|probe_indices': torch.Size([40]),\n",
       " 'NumType_Card|probe_classes': torch.Size([38]),\n",
       " 'VerbForm_Inf|probe_indices': torch.Size([42]),\n",
       " 'head': torch.Size([512]),\n",
       " 'Tense': torch.Size([512]),\n",
       " 'Person': torch.Size([512]),\n",
       " 'Gender_Masc|probe_indices': torch.Size([7]),\n",
       " 'VerbForm': torch.Size([512]),\n",
       " 'Gender_Neut|probe_classes': torch.Size([7])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{o:sample[o].shape if isinstance(sample[o],torch.Tensor) else sample[o] for o in other}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9355f697-1f44-4822-9358-41f019fe9cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|> Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border. [This killing of a respected cleric will be causing us trouble for years to come.] DPA: Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad. Two of them were being run by 2 officials of the Ministry of the Interior! The MoI in Iraq is equivalent to the US FBI, so this would be like having J. Edgar Hoover unwittingly employ at a high level members of the Weathermen bombers back in the 1960s. The third was being run by the head of an investment firm. You wonder if he was manipulating the market with his bombing targets. The cells were operating in the Ghazaliyah and al-Jihad districts of the capital. Although the announcement was probably made to show progress in identifying and breaking up terror cells, I don't find the news that the Baathists continue to penetrate the Iraqi government very hopeful. It reminds me too much of the ARVN officers who were secretly working for the other side in Vietnam. Al-Zaman : Guerrillas killed a member of the Kurdistan Democratic Party after kidnapping him in Mosul. The police commander of Ninevah Province announced that bombings had declined 80 percent in Mosul, whereas there had been a big jump in the number of kidnappings. On Wednesday guerrillas had kidnapped a cosmetic surgeon and his wife while they were on their way home. In Suwayrah, Kut Province, two car bombs were discovered before they could be detonated. (Kut is in southeastern Iraq and has an overwhelmingly Shiite population, who are on the lookout for Baathist saboteurs and willingly turn them in. This willingness is the main difference in the number of bombings in the south as opposed to the center-north of the country.) In Baghdad Kadhim Talal Husain, assistant dean at the School of Education at Mustansiriyah University, was assassinated with his driver in the Salikh district. Guerrillas killed an engineer, Asi Ali, from Tikrit. They also killed Shaikh Hamid 'Akkab, a clan elder of a branch of the Dulaim tribe in Tikrit. His mother was also killed in the attack. Two other Dulaim leaders have been killed in the past week and a half.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c2bd8e7-b022-45ca-9444-e5497e8be8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
       "        -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  1, -1, -1,\n",
       "        -1, -1, -1, -1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['eos_True|probe_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5088eadc-149f-4fbb-9318-0b9e829fa1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = [sample['tokens'].shape[0] for sample in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7fcd45b9-de2a-47cc-8a18-f2e0b4e3edf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 512.0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(token_lens), max(token_lens), sum(token_lens) / len(token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "128024af-58f9-4353-a7e8-da137dcbfd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumType\n",
      "Mood\n",
      "Voice\n",
      "PronType\n",
      "eos\n",
      "within_compound_token_ix\n",
      "Number\n",
      "first_eos\n",
      "position\n",
      "upos\n",
      "max_compound_token_ix\n",
      "dep\n",
      "Gender\n",
      "head\n",
      "Tense\n",
      "Person\n",
      "VerbForm\n"
     ]
    }
   ],
   "source": [
    "for k in unique_keys:\n",
    "    if k == 'tokens':\n",
    "        continue\n",
    "    if isinstance(sample[k], torch.Tensor) and len(sample[k]) == 512:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd1c715d-cfc0-4bab-89c2-f2794a4eb99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([ 34,  52,  64,  88,  94, 103, 104, 106, 109, 113, 121, 125, 127, 151,\n",
       "         158, 174, 197, 213, 225, 227, 250, 273, 281, 288, 298, 316, 322, 334,\n",
       "         339, 360, 399, 420, 432, 455, 457, 462, 463, 498]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['NumType'], sample['NumType_Card|probe_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e5d2bbd-82e4-4f97-9ae4-ac82373e3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample['dep_root|probe_indices'], sample['upos_PROPN|probe_indices'], sample['VerbForm_Ger|probe_indices'], sample['Mood_Imp|probe_indices'], sample['Gender_Fem|probe_indices'], sample['dep_conj|probe_indices'], sample['PronType_Rel|probe_indices'], sample['dep_obj|probe_indices'], sample['VerbForm_Fin|probe_indices'], sample['dep_nsubj:pass|probe_indices'], sample['upos_SYM|probe_indices'], sample['upos_PUNC|probe_indices'], sample['PronType_Int|probe_indices'], sample['dep_parataxis|probe_indices'], sample['upos_ADJ|probe_indices'], sample['Number_Plur|probe_indices'], sample['dep_advmod|probe_indices'], sample['dep_case|probe_indices'], sample['Tense_Past|probe_indices'], sample['dep_aux|probe_indices'], sample['PronType_Prs|probe_indices'], sample['Gender_Neut|probe_indices'], sample['first_eos_True|probe_indices'], sample['upos_AUX|probe_indices'], sample['Person_1|probe_indices'], sample['dep_flat|probe_indices'], sample['dep_advcl|probe_indices'], sample['NumType_Card|probe_indices'], sample['upos_NUM|probe_indices'], sample['upos_ADV|probe_indices'], sample['dep_acl:relcl|probe_indices'], sample['dep_nmod|probe_indices'], sample['upos_DET|probe_indices'], sample['VerbForm_Part|probe_indices'], sample['Voice_Pass|probe_indices'], sample['dep_obl|probe_indices'], sample['upos_VERB|probe_indices'], sample['dep_xcomp|probe_indices'], sample['PronType_Art|probe_indices'], sample['dep_nummod|probe_indices'], sample['upos_ADP|probe_indices'], sample['Person_2|probe_indices'], sample['upos_SCONJ|probe_indices'], sample['dep_det|probe_indices'], sample['PronType_Dem|probe_indices'], sample['dep_cc|probe_indices'], sample['dep_compound|probe_indices'], sample['upos_X|probe_indices'], sample['upos_NOUN|probe_indices'], sample['dep_appos|probe_indices'], sample['dep_cop|probe_indices'], sample['dep_list|probe_indices'], sample['upos_INTJ|probe_indices'], sample['upos_CCONJ|probe_indices'], sample['upos_PRON|probe_indices'], sample['dep_aux:pass|probe_indices'], sample['Person_3|probe_indices'], sample['dep_nsubj|probe_indices'], sample['eos_True|probe_indices'], sample['VerbForm_Inf|probe_indices'], sample['dep_acl|probe_indices'], sample['dep_mark|probe_indices'], sample['Gender_Masc|probe_indices'], sample['dep_amod|probe_indices'], sample['dep_ccomp|probe_indices'], sample['dep_punct|probe_indices'], sample['dep_nmod:poss|probe_indices'], "
     ]
    }
   ],
   "source": [
    "for k in unique_keys:\n",
    "    if 'probe_indices' in k:\n",
    "        print(f\"sample['{k}'],\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "57ade7ee-07e7-4663-b7de-c855bb528207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 23,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 38,\n",
       " 40,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 64,\n",
       " 65,\n",
       " 67,\n",
       " 70,\n",
       " 71,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 78,\n",
       " 80,\n",
       " 81,\n",
       " 84,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 92,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 112,\n",
       " 113,\n",
       " 115,\n",
       " 116,\n",
       " 119,\n",
       " 121,\n",
       " 122,\n",
       " 125,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 135,\n",
       " 138,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 151,\n",
       " 154,\n",
       " 155,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 166,\n",
       " 170,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 181,\n",
       " 183,\n",
       " 185,\n",
       " 186,\n",
       " 188,\n",
       " 193,\n",
       " 194,\n",
       " 196,\n",
       " 197,\n",
       " 201,\n",
       " 202,\n",
       " 204,\n",
       " 206,\n",
       " 208,\n",
       " 209,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 219,\n",
       " 221,\n",
       " 222,\n",
       " 224,\n",
       " 225,\n",
       " 227,\n",
       " 229,\n",
       " 234,\n",
       " 237,\n",
       " 238,\n",
       " 240,\n",
       " 241,\n",
       " 243,\n",
       " 244,\n",
       " 247,\n",
       " 248,\n",
       " 250,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 265,\n",
       " 267,\n",
       " 268,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 276,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 295,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 302,\n",
       " 303,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 309,\n",
       " 310,\n",
       " 312,\n",
       " 313,\n",
       " 316,\n",
       " 319,\n",
       " 320,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 343,\n",
       " 344,\n",
       " 350,\n",
       " 351,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 360,\n",
       " 362,\n",
       " 363,\n",
       " 365,\n",
       " 366,\n",
       " 368,\n",
       " 369,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 379,\n",
       " 380,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 387,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 405,\n",
       " 406,\n",
       " 408,\n",
       " 409,\n",
       " 411,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 424,\n",
       " 427,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 444,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 460,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 467,\n",
       " 469,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 482,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 495,\n",
       " 496,\n",
       " 498,\n",
       " 499,\n",
       " 500]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(torch.cat([sample['dep_root|probe_indices'], sample['upos_PROPN|probe_indices'], sample['VerbForm_Ger|probe_indices'], sample['Mood_Imp|probe_indices'], sample['Gender_Fem|probe_indices'], sample['dep_conj|probe_indices'], sample['PronType_Rel|probe_indices'], sample['dep_obj|probe_indices'], sample['VerbForm_Fin|probe_indices'], sample['dep_nsubj:pass|probe_indices'], sample['upos_SYM|probe_indices'], sample['upos_PUNC|probe_indices'], sample['PronType_Int|probe_indices'], sample['dep_parataxis|probe_indices'], sample['upos_ADJ|probe_indices'], sample['Number_Plur|probe_indices'], sample['dep_advmod|probe_indices'], sample['dep_case|probe_indices'], sample['Tense_Past|probe_indices'], sample['dep_aux|probe_indices'], sample['PronType_Prs|probe_indices'], sample['Gender_Neut|probe_indices'], sample['first_eos_True|probe_indices'], sample['upos_AUX|probe_indices'], sample['Person_1|probe_indices'], sample['dep_flat|probe_indices'], sample['dep_advcl|probe_indices'], sample['NumType_Card|probe_indices'], sample['upos_NUM|probe_indices'], sample['upos_ADV|probe_indices'], sample['dep_acl:relcl|probe_indices'], sample['dep_nmod|probe_indices'], sample['upos_DET|probe_indices'], sample['VerbForm_Part|probe_indices'], sample['Voice_Pass|probe_indices'], sample['dep_obl|probe_indices'], sample['upos_VERB|probe_indices'], sample['dep_xcomp|probe_indices'], sample['PronType_Art|probe_indices'], sample['dep_nummod|probe_indices'], sample['upos_ADP|probe_indices'], sample['Person_2|probe_indices'], sample['upos_SCONJ|probe_indices'], sample['dep_det|probe_indices'], sample['PronType_Dem|probe_indices'], sample['dep_cc|probe_indices'], sample['dep_compound|probe_indices'], sample['upos_X|probe_indices'], sample['upos_NOUN|probe_indices'], sample['dep_appos|probe_indices'], sample['dep_cop|probe_indices'], sample['dep_list|probe_indices'], sample['upos_INTJ|probe_indices'], sample['upos_CCONJ|probe_indices'], sample['upos_PRON|probe_indices'], sample['dep_aux:pass|probe_indices'], sample['Person_3|probe_indices'], sample['dep_nsubj|probe_indices'], sample['eos_True|probe_indices'], sample['VerbForm_Inf|probe_indices'], sample['dep_acl|probe_indices'], sample['dep_mark|probe_indices'], sample['Gender_Masc|probe_indices'], sample['dep_amod|probe_indices'], sample['dep_ccomp|probe_indices'], sample['dep_punct|probe_indices'], sample['dep_nmod:poss|probe_indices']]).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6321bca-032d-40a7-a38b-369aa146429b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ca6f01-6b6b-47bd-ad12-49fa5e88d166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = peek[9]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4798b753-85b9-40dc-86fb-cc2249097aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = data.select(list(range(1000,1438)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17fa9d0-4531-4c0a-9017-ccd1f7cc8c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([    0,   326,  6351,  3486,   949,   253, 35240,   789,   908,   281,\n",
       "         17093,   731,    15,   187,   187,    34, 19718, 16222,   281,  1445,\n",
       "           275,   399,   373,  3354,  3936,   441,   896,   247,  3213,   275,\n",
       "           673,    13,   281,   253,  1107,   816,  1078,   285,   846,  3645,\n",
       "          3660,   309,    13,   672,   253,  2846,   369,  1728,   281,   247,\n",
       "          1387,   273, 10846,   665,  1925,  3746,  9362,  2652,  3090, 33144,\n",
       "            13,   253, 15454,    15,  2596,   273,   616,  7342,   369,   281,\n",
       "         16497,  1270,  5685,  1445,   273,   253,  2469,  1905,  1219,  3381,\n",
       "         12914,   399,  3090,  6554,    13, 22838, 45374,   607,  1905,   715,\n",
       "           253,  3448,   273,  1246,    15,   496,   253,  1232,    13,   597,\n",
       "         14257, 23179, 18299,  1204,    15,   187,   187,   688,   253, 18471,\n",
       "            84,    13,   597,   574,  2323,    28,   368,   755,   247,  3282,\n",
       "           273,   436,   275, 41888, 45681, 20799,   348,  1216]),\n",
       " 'name': 'Ernst Ludwig Kirchner',\n",
       " 'text': ' that gain impact through the astonishing work used to illustrate them.\\n\\nA gallery devoted to art in Dresden takes us back a step in time, to the years just before and after World War I, when the city was home to a group of artists who called themselves Die Brücke, the Bridge. One of their goals was to translate great German art of the past — Albrecht Dürer, Lucas Cranach — into the language of present. In the process, they virtually invented Expressionism.\\n\\nIn the 1920s, they had success; you get a sense of this in Ernst Ludwig Kirchner',\n",
       " 'name_index_start': 123,\n",
       " 'name_index_end': 127,\n",
       " 'surname_index_start': 124,\n",
       " 'surname_index_end': 127,\n",
       " 'class': 'male'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sub[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1716f56b-c528-44d2-a042-a30b991598a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([    0,   326,  6351,  3486,   949,   253, 35240,   789,   908,   281,\n",
       "         17093,   731,    15,   187,   187,    34, 19718, 16222,   281,  1445,\n",
       "           275,   399,   373,  3354,  3936,   441,   896,   247,  3213,   275,\n",
       "           673,    13,   281,   253,  1107,   816,  1078,   285,   846,  3645,\n",
       "          3660,   309,    13,   672,   253,  2846,   369,  1728,   281,   247,\n",
       "          1387,   273, 10846,   665,  1925,  3746,  9362,  2652,  3090, 33144,\n",
       "            13,   253, 15454,    15,  2596,   273,   616,  7342,   369,   281,\n",
       "         16497,  1270,  5685,  1445,   273,   253,  2469,  1905,  1219,  3381,\n",
       "         12914,   399,  3090,  6554,    13, 22838, 45374,   607,  1905,   715,\n",
       "           253,  3448,   273,  1246,    15,   496,   253,  1232,    13,   597,\n",
       "         14257, 23179, 18299,  1204,    15,   187,   187,   688,   253, 18471,\n",
       "            84,    13,   597,   574,  2323,    28,   368,   755,   247,  3282,\n",
       "           273,   436,   275, 41888, 45681, 20799,   348,  1216]),\n",
       " 'name': 'Ernst Ludwig Kirchner',\n",
       " 'text': ' that gain impact through the astonishing work used to illustrate them.\\n\\nA gallery devoted to art in Dresden takes us back a step in time, to the years just before and after World War I, when the city was home to a group of artists who called themselves Die Brücke, the Bridge. One of their goals was to translate great German art of the past — Albrecht Dürer, Lucas Cranach — into the language of present. In the process, they virtually invented Expressionism.\\n\\nIn the 1920s, they had success; you get a sense of this in Ernst Ludwig Kirchner',\n",
       " 'name_index_start': 123,\n",
       " 'name_index_end': 127,\n",
       " 'surname_index_start': 124,\n",
       " 'surname_index_end': 127,\n",
       " 'class': 'male'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5252e3ad-b189-43f4-98f1-ec4470110381",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef9d762-36ff-4159-bd76-e06420c7a09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302d812-9373-40e4-a3c7-fa896b31d1a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sparsify Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82090f22-a5e9-49c8-aa00-b3e6f5b07776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsify.sparsify import Sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdf12fb-b066-4592-9e8b-719855f53803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b34a05ccc343789d03c8894c0e8ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 49 files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n"
     ]
    }
   ],
   "source": [
    "saes = Sae.load_many('EleutherAI/sae-pythia-410m-65k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5943cb46-d29c-4380-a618-804bd1a668e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layers.0.mlp', 'layers.1.mlp', 'layers.2.mlp', 'layers.3.mlp', 'layers.4.mlp', 'layers.5.mlp', 'layers.6.mlp', 'layers.7.mlp', 'layers.8.mlp', 'layers.9.mlp', 'layers.10.mlp', 'layers.11.mlp', 'layers.12.mlp', 'layers.13.mlp', 'layers.14.mlp', 'layers.15.mlp', 'layers.16.mlp', 'layers.17.mlp', 'layers.18.mlp', 'layers.19.mlp', 'layers.20.mlp', 'layers.21.mlp', 'layers.22.mlp', 'layers.23.mlp'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6389d532-41b3-4ffc-afae-751e9bbdf875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d901f3110540c6abe097269b9cc925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    }
   ],
   "source": [
    "sae = Sae.load_from_hub(\"EleutherAI/sae-pythia-410m-65k\", hookpoint=\"layers.22.mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1bcc22-2c3c-4bf5-9d4b-b3e601a6c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = sae.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754a349-b95f-4986-ae26-3fa4f6fe51c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the expected tensor shape in the sae forward method. It is: (inputs, input_embedding) i.e. (batch, seq, embedding) must be reduced to (batch * seq, embedding) before passing through forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504399d6-e7bc-4ef6-9430-a01dbbf35c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((16,18,1024), device='cuda:0')\n",
    "b = a.view(-1,1024).clone()\n",
    "b_op = sae.forward(b)\n",
    "c = b_op.sae_out.view(16,-1,1024)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dfe8bc0-daca-4a2d-a6df-7aa2ab27a16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(a.shape[1]):\n",
    "    temp_a = a[:,i,:]\n",
    "    #print('Temp A',temp_a.shape)\n",
    "    all_results.append(sae.forward(temp_a).sae_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6a07e3-0825-4070-938f-7fd06a91012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 18, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_prime = torch.stack(all_results,dim=1)\n",
    "c_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acbaf0bb-09e9-447b-a1ef-7162b4e75d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(c, c_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc4bea-9e46-48d6-a7e5-c0444d2d418a",
   "metadata": {},
   "source": [
    "## Testing how (batch, seq, embedding) can be translated into (n, embedding) and back to the (batch, seq, embedding) to maintain position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcc523-5e6c-4a49-b0f0-b25171157c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8dfab2-f186-40f7-b17f-381b1889fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f81e08c3-bb69-4e9b-a49d-909893dfd815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce718c76-992f-4404-bc27-7ca70736685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP shape torch.Size([16, 557, 1024])\n",
      "IP_Mod shape torch.Size([8912, 1024])\n",
      "IP_Prime shape torch.Size([16, 557, 1024])\n",
      "IP == IP_Prime: True\n",
      "_________________________\n",
      "Top Activations: torch.Size([8912, 32]), Top Activations Indices: torch.Size([8912, 32])\n",
      "Top Indices: tensor([ 4353,  8129, 10690, 15770, 15923, 17287, 17367, 26211, 29227, 29277,\n",
      "        32894, 33289, 33903, 36181, 37275, 38507, 38586, 39625, 42146, 42744,\n",
      "        42775, 48749, 49437, 50523, 51627, 51760, 52517, 52868, 56322, 57425,\n",
      "        58835, 28422], device='cuda:0')\n",
      "tensor([ 8.5520,  4.3722, 10.2342,  5.5670,  5.0898,  4.8645,  6.0769,  4.6501,\n",
      "         4.7437,  4.3074,  6.3670,  4.3509,  4.7134,  5.8097,  4.8363,  5.1812,\n",
      "         4.0863,  4.1682,  6.6390,  5.7630,  6.7948,  4.7071,  4.0192,  4.0508,\n",
      "         4.9415,  6.9674,  5.1510,  4.4646,  4.0666,  5.0733,  4.1196,  3.9472],\n",
      "       device='cuda:0')\n",
      "_________________________\n",
      "OP_Mod Shape torch.Size([8912, 1024])\n",
      "OP Shape torch.Size([16, 557, 1024])\n"
     ]
    }
   ],
   "source": [
    "batch = 16\n",
    "\n",
    "ip = torch.rand((batch, torch.randint(100,1000,size=(1,)).item(), 1024), device='cuda:0') # batch, sequence, embedding\n",
    "print('IP shape', ip.shape)\n",
    "ip_mod = einops.rearrange(ip, \"b s e -> (b s) e\")\n",
    "print('IP_Mod shape', ip_mod.shape)\n",
    "ip_prime = einops.rearrange(ip_mod, \"(b s) e -> b s e\", b = batch)\n",
    "print('IP_Prime shape', ip_prime.shape)\n",
    "print('IP == IP_Prime:', torch.allclose(ip_prime, ip))\n",
    "print('_'*25)\n",
    "with torch.no_grad():\n",
    "    enc = sae.encode(ip_mod)\n",
    "    print(f\"Top Activations: {enc.top_acts.shape}, Top Activations Indices: {enc.top_indices.shape}\")\n",
    "    print(\"Top Indices:\", enc.top_indices[0,:])\n",
    "    print(\"Top Activations\", enc.top_acts[0,:])\n",
    "    print('_'*25)\n",
    "    op_mod = sae.decode(*enc)\n",
    "    print(\"OP_Mod Shape\",op_mod.shape)\n",
    "    op = einops.rearrange(ip_mod, \"(b s) e -> b s e\", b = 16)\n",
    "    print(\"OP Shape\", op.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d127d25b-66f9-45f3-84ba-542aa113ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 32]), torch.Size([8192, 32]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2c2d20a-5637-495c-ab16-71011ad880f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  475,  2200,  3976,  4078,  5535,  5780,  6660,  6829,  9490,  9783,\n",
       "        10334, 10656, 11203, 11358, 11713, 12433, 14352, 16603, 19383, 19867,\n",
       "        20635, 21283, 24260, 24993, 28392, 28526, 30227, 30303, 30336, 31305,\n",
       "        31800,  6613], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e3710b5-9245-4599-94e6-5321918bc082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7005,  1.7695,  3.2423,  1.8509,  1.6629,  3.0474,  1.5182,  2.4735,\n",
       "         1.5209,  1.5189,  2.2650,  1.4994,  3.5307,  1.6637,  1.5109, 11.0463,\n",
       "         2.2309,  2.4835,  1.6456,  1.4765,  1.5123,  2.6469,  2.5875,  2.8152,\n",
       "         1.5593,  1.5475,  1.8769,  4.3661,  1.5953,  1.6941,  2.3382,  1.4640],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67e92b8e-94b1-4bd6-970d-e27784c6006b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5d653a4-0fe1-427b-bcb3-39c4f1613680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_acts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /home/mltrain/sparsify/sparsify/sparse_coder.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sae.decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6db0497d-4a38-4c25-92b6-040011f45024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-acts torch.Size([8752, 32768])\n",
      "Y torch.Size([8752, 768])\n",
      "sae_out torch.Size([8752, 768])\n",
      "E torch.Size([8752, 768])\n"
     ]
    }
   ],
   "source": [
    "dec = sae.forward(ip_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9b15327-db53-4a19-915c-cc240dd9778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.cfg.num_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e42c919b-2fe0-4bbe-9992-bdc4f3ee7717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8752, 768])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.sae_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1da865a2-78e1-42ff-a6aa-61930a2c92fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8752, 32])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.latent_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "188f93d6-3183-4b2b-ae90-46aa872682d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8752, 32])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.latent_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "451960b5-2c67-4579-9704-17491987880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = torch.zeros(8752, 32768, device=dec.latent_acts.device, dtype=dec.latent_acts.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c3476b33-c874-466e-a83f-5f28e2666814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8752, 32768])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25bfd1da-3931-47cb-b0a3-b1252d1cd1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.scatter_(1, dec.latent_indices, dec.latent_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aace8584-8f8d-445f-998b-34d2d741d1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8752, 32768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "862aeace-d8dc-4c32-8471-e829875be213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8752])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(32, device='cuda:0'), tensor(32, device='cuda:0'))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sum = (dense != 0).sum(dim=-1)\n",
    "print(p_sum.shape)\n",
    "p_sum.min(), p_sum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d1e211-59e8-427e-bc38-74eeb4c78dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythia_sae import SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5bc235-788b-4c60-a0a7-7618ed5b379b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c734fa95d841d588ec3fd354e71698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 laoded\n",
      "Layer 2 laoded\n",
      "Layer 3 laoded\n"
     ]
    }
   ],
   "source": [
    "sae_handle = SAE(device='cuda:0')\n",
    "sae_handle.load_many(\"EleutherAI/sae-pythia-160m-32k\", layers=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df34646-0682-4c13-a5f2-bd9874b48364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseCoder(\n",
       "  (encoder): Linear(in_features=768, out_features=32768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_handle.sae_layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0912d210-8459-4a98-8795-d09cfc3a95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-acts torch.Size([8192, 32768])\n",
      "Y torch.Size([8192, 768])\n",
      "sae_out torch.Size([8192, 768])\n",
      "E torch.Size([8192, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ip = torch.rand((16,512,768), device='cuda:0')\n",
    "op, sae_latents = sae_handle.compute_activations(ip, layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e3acb0-6264-4718-bc4a-b02f962595ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cc6569-0711-4162-ace9-3c612e98c891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 32768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6beba352-56bc-49d3-b156-fd0c04dd692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sum = (sae_latents != 0).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd571b7-e2bf-4ed9-b056-99b0e5243009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(32, device='cuda:0'), tensor(32, device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sum.min(), p_sum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6acfcf9e-e935-4b4c-a5b2-a1df740a4e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sample = sae_latents[0, [500,501],:]\n",
    "pos_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9298dec-39d4-46ab-87f8-62883b227e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = pos_sample.mean(dim=-1).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b6e515-9413-4936-a18e-cb991b41436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0081787109375, 0.007293701171875]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "273b927b-d38e-4767-b7f7-818c66429088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.eos_token)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbfd3a-4ce4-4927-98e1-91f614e8b0cf",
   "metadata": {},
   "source": [
    "## Shuffle and Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edc7c955-7b35-415d-850d-375de14d033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_select(*lists, n):\n",
    "    # Ensure there is at least one list\n",
    "    if not lists:\n",
    "        raise ValueError(\"At least one list must be provided.\")\n",
    "\n",
    "    # Check that all lists are of equal length\n",
    "    list_length = len(lists[0])\n",
    "    if any(len(lst) != list_length for lst in lists):\n",
    "        raise ValueError(\"All lists must have the same length.\")\n",
    "\n",
    "    # Zip the lists together to form a list of tuples.\n",
    "    combined = list(zip(*lists))\n",
    "    \n",
    "    # Shuffle the combined list in-place\n",
    "    random.shuffle(combined)\n",
    "    \n",
    "    # Select the first n elements\n",
    "    selected = combined[:n]\n",
    "    \n",
    "    # Unzip the selected tuples back into separate lists\n",
    "    # If n is 0, zip(*selected) would raise an error, so we handle that case.\n",
    "    if selected:\n",
    "        result_lists = list(map(list, zip(*selected)))\n",
    "    else:\n",
    "        # Return empty lists if no elements are selected\n",
    "        result_lists = [[] for _ in lists]\n",
    "    \n",
    "    return result_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2aad787-47bc-46a4-a950-0017deada664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "total = 8413\n",
    "select_n = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2749de73-71c6-4194-a8ea-6a73234f83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(total))\n",
    "random.shuffle(indices)\n",
    "indices = indices[:select_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "901a81d6-dd15-43fd-99cd-8e464e1a71fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 8411, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices), max(indices), min(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816756b6-0ba0-4f34-8667-4eddcadfe51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_select(model_activations, sae_activations, targets, total, select_n):\n",
    "    indices = list(range(total))[:select_n]\n",
    "    random.shuffle(indices)\n",
    "    for layer in model_activations.keys():\n",
    "        model_activations[layer] = [model_activations[layer][i] for i in indices]\n",
    "        sae_activations[layer] = [sae_activations[layer][i] for i in indices]\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
